<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Sannolikhetsteori och statistikteori | wlmr</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="Sannolikhetsteori och statistikteori" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&lt;!DOCTYPE html&gt; matematisk statistik matematisk statistik DEL 1: Sannolikhet eller hur man beskriver slumpen utfall – resultatet av ett slumpmässigt försök utfallsrummet – mängden möjliga utfall händelse – samling utfall relativ frekvens – kvoten mellan antalet erhållet utfall och hela antalet utförda kast disjunkta händelser – kan inte inträffa samtidigt Kolmogorovs axiomsystem: Händelsen P(A) måste ligga mellan 0 &amp; 1 P(utfallsrummet) = 1 om A &amp; B är parvis oförenliga gäller \( P(A) + P(B) = P(A \cup B) \) komplementsatsen P(A*) = 1 - P(A) Additionssatsen P(A or B) = P(A) + P(B) - P(A and B) Booles olikhet – \[ P(A \cup B) \leq P(A) + P(B) \] Kombinatorik Förutsättningar: n element k av dessa plockas Klassiska sannolikhetsdefinitionen Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall. Dragning med återläggning och hänsyn till ordning \[n^k\] Dragning utan återläggning med hänsyn till ordning \[n*(n-1)(n-2) \cdots (n-k)\] Dragning utan återläggning utan hänsyn till ordning \[\binom{n}{k}\] Dragning utan återläggning Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. Klas. sann. ges svaret av \[ g/m \] \[ m = \binom{v+s}{n} \] \[ g = \binom{v}{k} \binom{s}{n-k} \] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta. Dragning med återläggning Samma som ovan men med återläggning. \[ m = (v+s)^n \] \[ g = \binom{n}{k} v^k s^{n-k}\] Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m. Betingade sannolikheten – sannolikheten att något inträffar givet en annan händelse. \[P(B|A) = \frac{P(A \cap B)}{P(A)}\] Ger alltså ett samband mellan betingning och snitt. Lagen om total sannolikhet \[P(A) = \sum_{i=1}^{n} P(H_i)P(A|H_i)\] Bayes sats \[P(H_i|A) = \frac{P(H_i)P(A|H_i)}{\sum_{j=1}^{n} P(H_j)P(A|H_j)}\] Oberoende händelser – P(B|A) = P(B) sannolikheten att minst en inträffar \[A_1 , A_2 , … , A_n är oberoende och P(A_i)=p_i\] \[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\] 1. Endimensionella stokastiska variabler Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z. diskret stokastisk variabel En s.v. är diskret om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen. Enpunktsfördelning – all massa i ett värde \[p_X(a) = 1\] Tvåpunktsfördelning – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1-p. ex: krona/klave då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad. Likformig fördelning – X antar värden 1,2,..,m och alla dessa med samma sannolikhet. \[p_X(k)=1/m, k = 1,2,…,m.\] För-första-gången-fördelning \[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\] När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning. \[ X \in ffg(p) \] Geometrisk fördelning – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p) \[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\] Binomialfördelning – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr. \[ p_X(k) = \binom{n}{k}pk(1-p){n-k} \] \[ X \in Bin(n,p) \] Hypergeometrisk fördelning – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor. \[ p_X(k) = \frac{\binom{v}{k} \binom{s}{n-k}}{\binom{v+s}{n}}\] \[ X \in Hyp(N,n,p)\] Poisson-fördelning \[ p_X(k) = \frac{\muk}{k!}e{-e}\] \[ X \in Po(\mu)\] kontinuerlig stokastisk variabel Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f. Likformig fördelning – \[ f_X(x) = 1/(b-a) om a &lt; x &lt; b &gt;\] Exponentialfördelning – \[ f_X(x) = \lambda e^{-\lambda x} \] Normalfördelningen Weibull-fördelning Gammafördelning fördelningsfunktion intensitet 2. Flerdimensionella stokastiska variabler Note to self: \[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \] OBS! Dont forget the last P(i) Största och minsta värdet Z = max(X,Y) \[F_Z(z) = F_X(z)F_Y(z)\] Z = min(X,Y) \[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\] Gör först om till fördelningsfunktion om täthetsfunktion Summan av s.v. \[f_Z(z) = \int_{\infty}^{\infty} f_X(x)f_Y(z-x)dx\] 3. Väntevärden Note to self: konstanter inuti väntevärdesfunktioner är korkat. Väntevärdet E(X) eller μ Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade resultatet blir. DEF: \[ E(X) = \sum_kkp_X(k) \] \[ E(X) = \int\limits_{-\infty}^{\infty} x f_X(x)dx \] Y = g(X) \[ E(Y) = \sum_kg(k)p_X(k)\] \[ E(X+Y) = E(X)+E(Y) \] X &amp; Y oberoende \[ E(XY) = E(X)E(Y)\] Samling X med samma väntevärde µ \[ E(\sum_{i=1}^nX_i)=n\mu \] Betingade väntevärden \[ E(X|Y=k) = \sum_{j=0}^\infty jp_{X|Y=k}(j)\] \[ E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y=y}(x)dx\] Variansen V(X) eller σ² Variansen är en typ av spridningsmått. DEF: \[ V(X) = E[(X-\mu)^2] \] – alltså ett väntevärde LOL \[ V(X) = E(X^2)-[E(X)]^2 \] \[ V(aX+b) = a^2V(X) \] V(X + Y) = V(X) + V(Y) + 2C(X,Y) Om oberoende: V(X + Y) = V(X) + V(Y) Om oberoende och med samma σ: \[ V(\sum_{i=1}^nX_i) = n\sigma^2 \] Om oberoende och med samma σ samt µ: \[ V(\bar{X})=\sigma^2/n \] Standardavvikelse D(X) eller σ Schyst mått då man får samma dimension som väntevärdet \[ D(X) = \sqrt{V(X)} \] \[ D(aX + b) = |a|D(X) \] Om oberoende: \[ D(X + Y) = \sqrt{D^2(X) + D^2(Y)} \] Variationskoefficienten uttrycks i procent \[ R(X) = D(X)/E(X) \] fel systematiskt fel/bias är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal) slumpmässigt fel menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0) Beroendemått Kovarians Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden. \( C(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \) \( C(X,Y) = E(XY)-E(X)E(Y) \) Om C(X,Y) = 0 är X och Y okorrelerade. \( X \text{ &amp; } Y \text{ oberoende} \to \text{okorrelerade} \) Korrelationskoefficienten DEF: \[ \rho(X,Y) = \frac{C(X,Y)}{D(X)D(Y)} \] Kovarians fast dimensionslös Stora talens lag Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ. Betingade väntevärden och varianser Gauss approximationsformler Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen! En variabel taylorutveckla: \[ g(X) \approx g(\mu) + (X - \mu)g’(\mu) \] g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ². Flera variabler Abandon all hope, ye who enter here taylorutveckla: \[ g(X,Y) \approx g(\mu_X,\mu_Y)+(X-\mu_X)g’_X(\mu_X,\mu_Y)+(Y-\mu_Y)g’_Y(\mu_X,\mu_Y) \] 4. Normalfördelningen Notes to self: ca en tredjedel av massan hamnar utanför en standardavvikelse. normalfördelningar bevaras alltid under linjära transformationer \[ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \] Standardiserad fördelning Täthetsfunktion: φ Fördelningsfunktion: Φ \[ \Phi(-x) = 1 - \Phi(x) \] Allmän fördelning \[ X \in N(\mu,\sigma) \quad iff \quad Y = (X-\mu)/\sigma \in N(0,1) \] \[ f_X(x) = \frac{1}{\sigma}\varphi(\frac{x-\mu}{\sigma}) \] \[ F_X(x) = \Phi(\frac{x-\mu}{\sigma}) \] Linjärkombinationer Om \[ X \in N(\mu,\sigma) \] så gäller att \[ Y=aX+b \in N(a\mu + b, |a|\sigma) \] Om \[ X_1,X_2,..,X_n \] är oberoende N(µ,σ) och \[ \bar{X} \] är medelvärdet så gäller att \[ \bar{X} \in N(\mu,\sigma/\sqrt{n}) \] 5. De tre vännerna och Binomialfördelning binomialaren (med återläggning) E(X) = np V(X) = npq Om oberoende \[ X \in Bin(n_1,p) \quad \&amp; Y \in Bin(n_2,p) \] \[ X + Y \in Bin(n_1+n_2,p) \] Obs! Glöm inte att bin är diskret, håll därför koll på gränserna (&gt; != &gt;=) Kan approximeras som poissonfördelning om p är litet normalfördelning om n är stort N(np,sqrt(npq)) Hypergeometriske (utan återläggning) E(X) = np V(X) = ((N-n)/(N-1))np(1-p) Kan aproximeras som binomialapproximation om n/N är liten normalapproximation om n är stort Poisson-fördelningen E(X) = µ V(X) = µ \[ X_1 \in Po(\theta_1) \quad and \quad X_2 \in Po(\theta_2) \quad then \quad X_1+X_2 \in Po(\theta_1+\theta_2) \] Kan approximeras som normalfördelning om µ är stort Multinomial 6. Slumptal Markovkedjor Stokastiska processer vars nästa värde endast beror på nuvarande värde. övergångsmatris används för att skriva upp “hoppsannolikheterna”. övergångssannolikheter av 2a ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv. alltså sannolikheten att mellanlanda i ett tillstånd. För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor \[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \] \[ \begin{pmatrix} 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} 1to1 &amp; 1to2 \ 2to1 &amp; 2to2 \end{pmatrix} \] terminologi beständigt tillstånd om P(i-&gt;i)=1 obeständigt tillstånd om P(i-&gt;i) less than 1 Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte. irreducibel om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också. stationär fördelning sannolikheterna att systemet befinner sig i de olika tillstånden. skapa sannolikhetsvektorn π = (π1,π2,..) lös ekv. π = πP (P är övergångsmatrisen) när \[ p^{(n)}=(p_1^{(n)},p_2^{(n)},...) \to \pi \] när \[ n \to \infty \] 1. om man i en ändlig kedja kan finna ett r&gt;0 så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en asymptotisk fördelning. 2. se stationär fördelning periodiska tillstånd om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3. aperiodiska tillstånd om det alltid går att komma tillbaka till ett tillstånd direkt DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial terminologi parameterrummet - de värden den sökta parametern kan tänkas anta. stickprov – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v. stickprovsvariansen \[ s^2 = \frac{1}{n-1} \sum_{j=1}^n (x_j - \bar{x})^2 \] kovariansen mellan x- och y-värdena i en datamängd (x1,y1),(x2,y2),…,(xn,yn) \[ c_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \] korrelationskoefficienten \[ r = \frac{c_{xy}}{s_xs_y} \] 7. Punktskattning punktskattning – den observerade sannolikheten – ett utfall av stickprovsvariabeln \[ \theta_{obs}^*(x_1,x_2,…,x_n) \] stickprovsvariabeln – en s.v. som punktskattningen är ett utfall av \[ \theta^*(X_1,X_2,…,X_n) \] väntevärdesriktig – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \[ E(\theta^*) = \theta \] MSE – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \[ MSE = E(( \theta^* - \theta)^2) \] skattning av μ &amp; σ µ stickprovsmedelvärdet \[ \bar{x} \] är en väntevärdesriktig och konsistent skattning av µ σ^2 stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2 Maximum-likelihood-metoden – ML-metoden Skapa \[ L(\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\theta) \] alt. \[ L(\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\theta) \] (A.k.a. likelihood-funktionen) Finn funktionens maxpunkt genom ex. derivering över theta. Funktionens största värde är det mest sannolika scenariot. Minsta-kvadrat-metoden – MK-metoden \[ Q(\theta) = \sum_{i=1}^n [x_i - \mu_i (\theta)]^2 \] Går ut på att anta att det finns små försöksfel vid varje mätdatum och bara genom att minimera dessa finner man bästa skattning av theta. Tillämpning på normalfördelningen Ett stickprov µ okänt σ känt \[\mu* = \bar{x} \] μ känt σ okänt \[ (\sigma^2)_{obs}^* = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \] Konfidensintervall för väntevärdet Känd standardavvikelse En lämplig skattning av µ är aritmetiska medelvärdet av X. \[ \bar{X} \in N(\mu,D) \] \[ D = \sigma/\sqrt{n} \] \[ I_\mu = (\bar{x}-\lambda_{\alpha/2}D,\bar{x}+\lambda_{\alpha/2}D) \] Allt detta följer av att: \[ \frac{\bar{X}-\mu}{D} \in N(0,1) \] Följaktligen gäller med sannolikheten 1-alfa att: \[ -\lambda_{\alpha/2} &lt; \frac{\bar{X}-\mu}{D} &lt; \lambda_{\alpha/2} &gt;\] Om vi har ett intervall: \[ I_\mu = (16 \pm 2.58 * 0.155) \] där \[ D = 1.2/\sqrt{60} = 0.155 \] och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation: \[ 2 * 2.58 * 1.2/\sqrt{n} = 0.5 \] Okänd standardavvikelse I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ \[ I_\mu = (\bar{x}-t_{\alpha/2}(f)d,\bar{x}+t_{\alpha/2}(f)d) \] \[ d = s/\sqrt{n}, \quad f = n-1 \] Konfidensintervall för standardavvikelsen μ känt Aint gonna happen gurl µ okänt \[ I_\sigma = (k_1s,k_2s) \] \[ k_1 = \sqrt{(f/\chi_{\alpha/2}^2(f)} \] \[ k_2 = \sqrt{(f/\chi_{1-\alpha/2}^2(f)} \] \[ f = n-1 \] Två stickprov Om σ1 och σ2 är kända: \[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-\lambda_{\alpha/2}D,\bar{x}-\bar{y}+\lambda_{\alpha/2}D) \] \[ D = \sqrt{ \sigma_{1}^{2} / n_1 + \sigma_{2}^{2} / n_2} \] Om σ1 = σ2 = σ: \[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-t_{\alpha/2}(f)d,\bar{x}-\bar{y}+t_{\alpha/2}(f)d) \] \[ d = \sigma \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}} \] Stickprov i par Skapa \[ z = y - x \] 8. Intevallskattning När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval. Tillämpning på normalfördelningen Känd standardavvikelse 9. Hypotesprövning nollhypotes – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \( H_0 \) mothypotes – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \( H_i \) signifikansnivå/felrisk – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre). signifikant* – 0.05 signifikant** – 0.01 signifikant*** – 0.001 testvariabel/teststorhet – observation av stickprovsvariabel signifikanstest styrkefunktionen \[ h(\theta) = P(H_0 \text{ förkastas}) \] om θ är det rätta värdet bör vara stort för alla θ som tillhör mothypotesen bör vara litet för alla θ som tillhör nollhypotesen h(θ) kallas testets styrka för θ konfidensmetoden – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför 10. Regressionsanalys När man vill se samband mellan två eller flera storheter. Terminologi teoretiska regressionslinjen \[ y = \alpha + \beta x \] Punktskattningar Remember MK-metoden? Bestäm minimum för \[ Q( \alpha , \beta ) = \sum_{i}^n (y_i - \mu_i)^2 \] \[ \mu_i = \alpha + \beta x_i \] Genom att sätta partialderivatorna till noll fås \[ \beta^* = \frac{S_{xy}}{S_{xx}} \quad \alpha^* = \bar{y} - \beta^* \bar{x} \] Intervallskattningar 11. Fallgropar" />
<meta property="og:description" content="&lt;!DOCTYPE html&gt; matematisk statistik matematisk statistik DEL 1: Sannolikhet eller hur man beskriver slumpen utfall – resultatet av ett slumpmässigt försök utfallsrummet – mängden möjliga utfall händelse – samling utfall relativ frekvens – kvoten mellan antalet erhållet utfall och hela antalet utförda kast disjunkta händelser – kan inte inträffa samtidigt Kolmogorovs axiomsystem: Händelsen P(A) måste ligga mellan 0 &amp; 1 P(utfallsrummet) = 1 om A &amp; B är parvis oförenliga gäller \( P(A) + P(B) = P(A \cup B) \) komplementsatsen P(A*) = 1 - P(A) Additionssatsen P(A or B) = P(A) + P(B) - P(A and B) Booles olikhet – \[ P(A \cup B) \leq P(A) + P(B) \] Kombinatorik Förutsättningar: n element k av dessa plockas Klassiska sannolikhetsdefinitionen Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall. Dragning med återläggning och hänsyn till ordning \[n^k\] Dragning utan återläggning med hänsyn till ordning \[n*(n-1)(n-2) \cdots (n-k)\] Dragning utan återläggning utan hänsyn till ordning \[\binom{n}{k}\] Dragning utan återläggning Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. Klas. sann. ges svaret av \[ g/m \] \[ m = \binom{v+s}{n} \] \[ g = \binom{v}{k} \binom{s}{n-k} \] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta. Dragning med återläggning Samma som ovan men med återläggning. \[ m = (v+s)^n \] \[ g = \binom{n}{k} v^k s^{n-k}\] Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m. Betingade sannolikheten – sannolikheten att något inträffar givet en annan händelse. \[P(B|A) = \frac{P(A \cap B)}{P(A)}\] Ger alltså ett samband mellan betingning och snitt. Lagen om total sannolikhet \[P(A) = \sum_{i=1}^{n} P(H_i)P(A|H_i)\] Bayes sats \[P(H_i|A) = \frac{P(H_i)P(A|H_i)}{\sum_{j=1}^{n} P(H_j)P(A|H_j)}\] Oberoende händelser – P(B|A) = P(B) sannolikheten att minst en inträffar \[A_1 , A_2 , … , A_n är oberoende och P(A_i)=p_i\] \[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\] 1. Endimensionella stokastiska variabler Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z. diskret stokastisk variabel En s.v. är diskret om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen. Enpunktsfördelning – all massa i ett värde \[p_X(a) = 1\] Tvåpunktsfördelning – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1-p. ex: krona/klave då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad. Likformig fördelning – X antar värden 1,2,..,m och alla dessa med samma sannolikhet. \[p_X(k)=1/m, k = 1,2,…,m.\] För-första-gången-fördelning \[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\] När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning. \[ X \in ffg(p) \] Geometrisk fördelning – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p) \[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\] Binomialfördelning – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr. \[ p_X(k) = \binom{n}{k}pk(1-p){n-k} \] \[ X \in Bin(n,p) \] Hypergeometrisk fördelning – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor. \[ p_X(k) = \frac{\binom{v}{k} \binom{s}{n-k}}{\binom{v+s}{n}}\] \[ X \in Hyp(N,n,p)\] Poisson-fördelning \[ p_X(k) = \frac{\muk}{k!}e{-e}\] \[ X \in Po(\mu)\] kontinuerlig stokastisk variabel Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f. Likformig fördelning – \[ f_X(x) = 1/(b-a) om a &lt; x &lt; b &gt;\] Exponentialfördelning – \[ f_X(x) = \lambda e^{-\lambda x} \] Normalfördelningen Weibull-fördelning Gammafördelning fördelningsfunktion intensitet 2. Flerdimensionella stokastiska variabler Note to self: \[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \] OBS! Dont forget the last P(i) Största och minsta värdet Z = max(X,Y) \[F_Z(z) = F_X(z)F_Y(z)\] Z = min(X,Y) \[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\] Gör först om till fördelningsfunktion om täthetsfunktion Summan av s.v. \[f_Z(z) = \int_{\infty}^{\infty} f_X(x)f_Y(z-x)dx\] 3. Väntevärden Note to self: konstanter inuti väntevärdesfunktioner är korkat. Väntevärdet E(X) eller μ Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade resultatet blir. DEF: \[ E(X) = \sum_kkp_X(k) \] \[ E(X) = \int\limits_{-\infty}^{\infty} x f_X(x)dx \] Y = g(X) \[ E(Y) = \sum_kg(k)p_X(k)\] \[ E(X+Y) = E(X)+E(Y) \] X &amp; Y oberoende \[ E(XY) = E(X)E(Y)\] Samling X med samma väntevärde µ \[ E(\sum_{i=1}^nX_i)=n\mu \] Betingade väntevärden \[ E(X|Y=k) = \sum_{j=0}^\infty jp_{X|Y=k}(j)\] \[ E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y=y}(x)dx\] Variansen V(X) eller σ² Variansen är en typ av spridningsmått. DEF: \[ V(X) = E[(X-\mu)^2] \] – alltså ett väntevärde LOL \[ V(X) = E(X^2)-[E(X)]^2 \] \[ V(aX+b) = a^2V(X) \] V(X + Y) = V(X) + V(Y) + 2C(X,Y) Om oberoende: V(X + Y) = V(X) + V(Y) Om oberoende och med samma σ: \[ V(\sum_{i=1}^nX_i) = n\sigma^2 \] Om oberoende och med samma σ samt µ: \[ V(\bar{X})=\sigma^2/n \] Standardavvikelse D(X) eller σ Schyst mått då man får samma dimension som väntevärdet \[ D(X) = \sqrt{V(X)} \] \[ D(aX + b) = |a|D(X) \] Om oberoende: \[ D(X + Y) = \sqrt{D^2(X) + D^2(Y)} \] Variationskoefficienten uttrycks i procent \[ R(X) = D(X)/E(X) \] fel systematiskt fel/bias är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal) slumpmässigt fel menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0) Beroendemått Kovarians Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden. \( C(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \) \( C(X,Y) = E(XY)-E(X)E(Y) \) Om C(X,Y) = 0 är X och Y okorrelerade. \( X \text{ &amp; } Y \text{ oberoende} \to \text{okorrelerade} \) Korrelationskoefficienten DEF: \[ \rho(X,Y) = \frac{C(X,Y)}{D(X)D(Y)} \] Kovarians fast dimensionslös Stora talens lag Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ. Betingade väntevärden och varianser Gauss approximationsformler Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen! En variabel taylorutveckla: \[ g(X) \approx g(\mu) + (X - \mu)g’(\mu) \] g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ². Flera variabler Abandon all hope, ye who enter here taylorutveckla: \[ g(X,Y) \approx g(\mu_X,\mu_Y)+(X-\mu_X)g’_X(\mu_X,\mu_Y)+(Y-\mu_Y)g’_Y(\mu_X,\mu_Y) \] 4. Normalfördelningen Notes to self: ca en tredjedel av massan hamnar utanför en standardavvikelse. normalfördelningar bevaras alltid under linjära transformationer \[ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \] Standardiserad fördelning Täthetsfunktion: φ Fördelningsfunktion: Φ \[ \Phi(-x) = 1 - \Phi(x) \] Allmän fördelning \[ X \in N(\mu,\sigma) \quad iff \quad Y = (X-\mu)/\sigma \in N(0,1) \] \[ f_X(x) = \frac{1}{\sigma}\varphi(\frac{x-\mu}{\sigma}) \] \[ F_X(x) = \Phi(\frac{x-\mu}{\sigma}) \] Linjärkombinationer Om \[ X \in N(\mu,\sigma) \] så gäller att \[ Y=aX+b \in N(a\mu + b, |a|\sigma) \] Om \[ X_1,X_2,..,X_n \] är oberoende N(µ,σ) och \[ \bar{X} \] är medelvärdet så gäller att \[ \bar{X} \in N(\mu,\sigma/\sqrt{n}) \] 5. De tre vännerna och Binomialfördelning binomialaren (med återläggning) E(X) = np V(X) = npq Om oberoende \[ X \in Bin(n_1,p) \quad \&amp; Y \in Bin(n_2,p) \] \[ X + Y \in Bin(n_1+n_2,p) \] Obs! Glöm inte att bin är diskret, håll därför koll på gränserna (&gt; != &gt;=) Kan approximeras som poissonfördelning om p är litet normalfördelning om n är stort N(np,sqrt(npq)) Hypergeometriske (utan återläggning) E(X) = np V(X) = ((N-n)/(N-1))np(1-p) Kan aproximeras som binomialapproximation om n/N är liten normalapproximation om n är stort Poisson-fördelningen E(X) = µ V(X) = µ \[ X_1 \in Po(\theta_1) \quad and \quad X_2 \in Po(\theta_2) \quad then \quad X_1+X_2 \in Po(\theta_1+\theta_2) \] Kan approximeras som normalfördelning om µ är stort Multinomial 6. Slumptal Markovkedjor Stokastiska processer vars nästa värde endast beror på nuvarande värde. övergångsmatris används för att skriva upp “hoppsannolikheterna”. övergångssannolikheter av 2a ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv. alltså sannolikheten att mellanlanda i ett tillstånd. För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor \[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \] \[ \begin{pmatrix} 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} 1to1 &amp; 1to2 \ 2to1 &amp; 2to2 \end{pmatrix} \] terminologi beständigt tillstånd om P(i-&gt;i)=1 obeständigt tillstånd om P(i-&gt;i) less than 1 Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte. irreducibel om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också. stationär fördelning sannolikheterna att systemet befinner sig i de olika tillstånden. skapa sannolikhetsvektorn π = (π1,π2,..) lös ekv. π = πP (P är övergångsmatrisen) när \[ p^{(n)}=(p_1^{(n)},p_2^{(n)},...) \to \pi \] när \[ n \to \infty \] 1. om man i en ändlig kedja kan finna ett r&gt;0 så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en asymptotisk fördelning. 2. se stationär fördelning periodiska tillstånd om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3. aperiodiska tillstånd om det alltid går att komma tillbaka till ett tillstånd direkt DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial terminologi parameterrummet - de värden den sökta parametern kan tänkas anta. stickprov – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v. stickprovsvariansen \[ s^2 = \frac{1}{n-1} \sum_{j=1}^n (x_j - \bar{x})^2 \] kovariansen mellan x- och y-värdena i en datamängd (x1,y1),(x2,y2),…,(xn,yn) \[ c_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \] korrelationskoefficienten \[ r = \frac{c_{xy}}{s_xs_y} \] 7. Punktskattning punktskattning – den observerade sannolikheten – ett utfall av stickprovsvariabeln \[ \theta_{obs}^*(x_1,x_2,…,x_n) \] stickprovsvariabeln – en s.v. som punktskattningen är ett utfall av \[ \theta^*(X_1,X_2,…,X_n) \] väntevärdesriktig – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \[ E(\theta^*) = \theta \] MSE – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \[ MSE = E(( \theta^* - \theta)^2) \] skattning av μ &amp; σ µ stickprovsmedelvärdet \[ \bar{x} \] är en väntevärdesriktig och konsistent skattning av µ σ^2 stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2 Maximum-likelihood-metoden – ML-metoden Skapa \[ L(\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\theta) \] alt. \[ L(\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\theta) \] (A.k.a. likelihood-funktionen) Finn funktionens maxpunkt genom ex. derivering över theta. Funktionens största värde är det mest sannolika scenariot. Minsta-kvadrat-metoden – MK-metoden \[ Q(\theta) = \sum_{i=1}^n [x_i - \mu_i (\theta)]^2 \] Går ut på att anta att det finns små försöksfel vid varje mätdatum och bara genom att minimera dessa finner man bästa skattning av theta. Tillämpning på normalfördelningen Ett stickprov µ okänt σ känt \[\mu* = \bar{x} \] μ känt σ okänt \[ (\sigma^2)_{obs}^* = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \] Konfidensintervall för väntevärdet Känd standardavvikelse En lämplig skattning av µ är aritmetiska medelvärdet av X. \[ \bar{X} \in N(\mu,D) \] \[ D = \sigma/\sqrt{n} \] \[ I_\mu = (\bar{x}-\lambda_{\alpha/2}D,\bar{x}+\lambda_{\alpha/2}D) \] Allt detta följer av att: \[ \frac{\bar{X}-\mu}{D} \in N(0,1) \] Följaktligen gäller med sannolikheten 1-alfa att: \[ -\lambda_{\alpha/2} &lt; \frac{\bar{X}-\mu}{D} &lt; \lambda_{\alpha/2} &gt;\] Om vi har ett intervall: \[ I_\mu = (16 \pm 2.58 * 0.155) \] där \[ D = 1.2/\sqrt{60} = 0.155 \] och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation: \[ 2 * 2.58 * 1.2/\sqrt{n} = 0.5 \] Okänd standardavvikelse I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ \[ I_\mu = (\bar{x}-t_{\alpha/2}(f)d,\bar{x}+t_{\alpha/2}(f)d) \] \[ d = s/\sqrt{n}, \quad f = n-1 \] Konfidensintervall för standardavvikelsen μ känt Aint gonna happen gurl µ okänt \[ I_\sigma = (k_1s,k_2s) \] \[ k_1 = \sqrt{(f/\chi_{\alpha/2}^2(f)} \] \[ k_2 = \sqrt{(f/\chi_{1-\alpha/2}^2(f)} \] \[ f = n-1 \] Två stickprov Om σ1 och σ2 är kända: \[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-\lambda_{\alpha/2}D,\bar{x}-\bar{y}+\lambda_{\alpha/2}D) \] \[ D = \sqrt{ \sigma_{1}^{2} / n_1 + \sigma_{2}^{2} / n_2} \] Om σ1 = σ2 = σ: \[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-t_{\alpha/2}(f)d,\bar{x}-\bar{y}+t_{\alpha/2}(f)d) \] \[ d = \sigma \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}} \] Stickprov i par Skapa \[ z = y - x \] 8. Intevallskattning När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval. Tillämpning på normalfördelningen Känd standardavvikelse 9. Hypotesprövning nollhypotes – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \( H_0 \) mothypotes – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \( H_i \) signifikansnivå/felrisk – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre). signifikant* – 0.05 signifikant** – 0.01 signifikant*** – 0.001 testvariabel/teststorhet – observation av stickprovsvariabel signifikanstest styrkefunktionen \[ h(\theta) = P(H_0 \text{ förkastas}) \] om θ är det rätta värdet bör vara stort för alla θ som tillhör mothypotesen bör vara litet för alla θ som tillhör nollhypotesen h(θ) kallas testets styrka för θ konfidensmetoden – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför 10. Regressionsanalys När man vill se samband mellan två eller flera storheter. Terminologi teoretiska regressionslinjen \[ y = \alpha + \beta x \] Punktskattningar Remember MK-metoden? Bestäm minimum för \[ Q( \alpha , \beta ) = \sum_{i}^n (y_i - \mu_i)^2 \] \[ \mu_i = \alpha + \beta x_i \] Genom att sätta partialderivatorna till noll fås \[ \beta^* = \frac{S_{xy}}{S_{xx}} \quad \alpha^* = \bar{y} - \beta^* \bar{x} \] Intervallskattningar 11. Fallgropar" />
<link rel="canonical" href="http://localhost:4000/2018/06/20/statistik.html" />
<meta property="og:url" content="http://localhost:4000/2018/06/20/statistik.html" />
<meta property="og:site_name" content="wlmr" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-20T00:00:00+02:00" />
<script type="application/ld+json">
{"description":"&lt;!DOCTYPE html&gt; matematisk statistik matematisk statistik DEL 1: Sannolikhet eller hur man beskriver slumpen utfall – resultatet av ett slumpmässigt försök utfallsrummet – mängden möjliga utfall händelse – samling utfall relativ frekvens – kvoten mellan antalet erhållet utfall och hela antalet utförda kast disjunkta händelser – kan inte inträffa samtidigt Kolmogorovs axiomsystem: Händelsen P(A) måste ligga mellan 0 &amp; 1 P(utfallsrummet) = 1 om A &amp; B är parvis oförenliga gäller \\( P(A) + P(B) = P(A \\cup B) \\) komplementsatsen P(A*) = 1 - P(A) Additionssatsen P(A or B) = P(A) + P(B) - P(A and B) Booles olikhet – \\[ P(A \\cup B) \\leq P(A) + P(B) \\] Kombinatorik Förutsättningar: n element k av dessa plockas Klassiska sannolikhetsdefinitionen Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall. Dragning med återläggning och hänsyn till ordning \\[n^k\\] Dragning utan återläggning med hänsyn till ordning \\[n*(n-1)(n-2) \\cdots (n-k)\\] Dragning utan återläggning utan hänsyn till ordning \\[\\binom{n}{k}\\] Dragning utan återläggning Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. Klas. sann. ges svaret av \\[ g/m \\] \\[ m = \\binom{v+s}{n} \\] \\[ g = \\binom{v}{k} \\binom{s}{n-k} \\] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta. Dragning med återläggning Samma som ovan men med återläggning. \\[ m = (v+s)^n \\] \\[ g = \\binom{n}{k} v^k s^{n-k}\\] Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m. Betingade sannolikheten – sannolikheten att något inträffar givet en annan händelse. \\[P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\] Ger alltså ett samband mellan betingning och snitt. Lagen om total sannolikhet \\[P(A) = \\sum_{i=1}^{n} P(H_i)P(A|H_i)\\] Bayes sats \\[P(H_i|A) = \\frac{P(H_i)P(A|H_i)}{\\sum_{j=1}^{n} P(H_j)P(A|H_j)}\\] Oberoende händelser – P(B|A) = P(B) sannolikheten att minst en inträffar \\[A_1 , A_2 , … , A_n är oberoende och P(A_i)=p_i\\] \\[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\\] 1. Endimensionella stokastiska variabler Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z. diskret stokastisk variabel En s.v. är diskret om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen. Enpunktsfördelning – all massa i ett värde \\[p_X(a) = 1\\] Tvåpunktsfördelning – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1-p. ex: krona/klave då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad. Likformig fördelning – X antar värden 1,2,..,m och alla dessa med samma sannolikhet. \\[p_X(k)=1/m, k = 1,2,…,m.\\] För-första-gången-fördelning \\[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\\] När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning. \\[ X \\in ffg(p) \\] Geometrisk fördelning – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p) \\[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\\] Binomialfördelning – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr. \\[ p_X(k) = \\binom{n}{k}pk(1-p){n-k} \\] \\[ X \\in Bin(n,p) \\] Hypergeometrisk fördelning – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor. \\[ p_X(k) = \\frac{\\binom{v}{k} \\binom{s}{n-k}}{\\binom{v+s}{n}}\\] \\[ X \\in Hyp(N,n,p)\\] Poisson-fördelning \\[ p_X(k) = \\frac{\\muk}{k!}e{-e}\\] \\[ X \\in Po(\\mu)\\] kontinuerlig stokastisk variabel Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f. Likformig fördelning – \\[ f_X(x) = 1/(b-a) om a &lt; x &lt; b &gt;\\] Exponentialfördelning – \\[ f_X(x) = \\lambda e^{-\\lambda x} \\] Normalfördelningen Weibull-fördelning Gammafördelning fördelningsfunktion intensitet 2. Flerdimensionella stokastiska variabler Note to self: \\[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \\] OBS! Dont forget the last P(i) Största och minsta värdet Z = max(X,Y) \\[F_Z(z) = F_X(z)F_Y(z)\\] Z = min(X,Y) \\[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\\] Gör först om till fördelningsfunktion om täthetsfunktion Summan av s.v. \\[f_Z(z) = \\int_{\\infty}^{\\infty} f_X(x)f_Y(z-x)dx\\] 3. Väntevärden Note to self: konstanter inuti väntevärdesfunktioner är korkat. Väntevärdet E(X) eller μ Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade resultatet blir. DEF: \\[ E(X) = \\sum_kkp_X(k) \\] \\[ E(X) = \\int\\limits_{-\\infty}^{\\infty} x f_X(x)dx \\] Y = g(X) \\[ E(Y) = \\sum_kg(k)p_X(k)\\] \\[ E(X+Y) = E(X)+E(Y) \\] X &amp; Y oberoende \\[ E(XY) = E(X)E(Y)\\] Samling X med samma väntevärde µ \\[ E(\\sum_{i=1}^nX_i)=n\\mu \\] Betingade väntevärden \\[ E(X|Y=k) = \\sum_{j=0}^\\infty jp_{X|Y=k}(j)\\] \\[ E(X|Y=y) = \\int_{-\\infty}^\\infty xf_{X|Y=y}(x)dx\\] Variansen V(X) eller σ² Variansen är en typ av spridningsmått. DEF: \\[ V(X) = E[(X-\\mu)^2] \\] – alltså ett väntevärde LOL \\[ V(X) = E(X^2)-[E(X)]^2 \\] \\[ V(aX+b) = a^2V(X) \\] V(X + Y) = V(X) + V(Y) + 2C(X,Y) Om oberoende: V(X + Y) = V(X) + V(Y) Om oberoende och med samma σ: \\[ V(\\sum_{i=1}^nX_i) = n\\sigma^2 \\] Om oberoende och med samma σ samt µ: \\[ V(\\bar{X})=\\sigma^2/n \\] Standardavvikelse D(X) eller σ Schyst mått då man får samma dimension som väntevärdet \\[ D(X) = \\sqrt{V(X)} \\] \\[ D(aX + b) = |a|D(X) \\] Om oberoende: \\[ D(X + Y) = \\sqrt{D^2(X) + D^2(Y)} \\] Variationskoefficienten uttrycks i procent \\[ R(X) = D(X)/E(X) \\] fel systematiskt fel/bias är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal) slumpmässigt fel menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0) Beroendemått Kovarians Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden. \\( C(X,Y) = E[(X-\\mu_X)(Y-\\mu_Y)] \\) \\( C(X,Y) = E(XY)-E(X)E(Y) \\) Om C(X,Y) = 0 är X och Y okorrelerade. \\( X \\text{ &amp; } Y \\text{ oberoende} \\to \\text{okorrelerade} \\) Korrelationskoefficienten DEF: \\[ \\rho(X,Y) = \\frac{C(X,Y)}{D(X)D(Y)} \\] Kovarians fast dimensionslös Stora talens lag Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ. Betingade väntevärden och varianser Gauss approximationsformler Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen! En variabel taylorutveckla: \\[ g(X) \\approx g(\\mu) + (X - \\mu)g’(\\mu) \\] g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ². Flera variabler Abandon all hope, ye who enter here taylorutveckla: \\[ g(X,Y) \\approx g(\\mu_X,\\mu_Y)+(X-\\mu_X)g’_X(\\mu_X,\\mu_Y)+(Y-\\mu_Y)g’_Y(\\mu_X,\\mu_Y) \\] 4. Normalfördelningen Notes to self: ca en tredjedel av massan hamnar utanför en standardavvikelse. normalfördelningar bevaras alltid under linjära transformationer \\[ f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \\] Standardiserad fördelning Täthetsfunktion: φ Fördelningsfunktion: Φ \\[ \\Phi(-x) = 1 - \\Phi(x) \\] Allmän fördelning \\[ X \\in N(\\mu,\\sigma) \\quad iff \\quad Y = (X-\\mu)/\\sigma \\in N(0,1) \\] \\[ f_X(x) = \\frac{1}{\\sigma}\\varphi(\\frac{x-\\mu}{\\sigma}) \\] \\[ F_X(x) = \\Phi(\\frac{x-\\mu}{\\sigma}) \\] Linjärkombinationer Om \\[ X \\in N(\\mu,\\sigma) \\] så gäller att \\[ Y=aX+b \\in N(a\\mu + b, |a|\\sigma) \\] Om \\[ X_1,X_2,..,X_n \\] är oberoende N(µ,σ) och \\[ \\bar{X} \\] är medelvärdet så gäller att \\[ \\bar{X} \\in N(\\mu,\\sigma/\\sqrt{n}) \\] 5. De tre vännerna och Binomialfördelning binomialaren (med återläggning) E(X) = np V(X) = npq Om oberoende \\[ X \\in Bin(n_1,p) \\quad \\&amp; Y \\in Bin(n_2,p) \\] \\[ X + Y \\in Bin(n_1+n_2,p) \\] Obs! Glöm inte att bin är diskret, håll därför koll på gränserna (&gt; != &gt;=) Kan approximeras som poissonfördelning om p är litet normalfördelning om n är stort N(np,sqrt(npq)) Hypergeometriske (utan återläggning) E(X) = np V(X) = ((N-n)/(N-1))np(1-p) Kan aproximeras som binomialapproximation om n/N är liten normalapproximation om n är stort Poisson-fördelningen E(X) = µ V(X) = µ \\[ X_1 \\in Po(\\theta_1) \\quad and \\quad X_2 \\in Po(\\theta_2) \\quad then \\quad X_1+X_2 \\in Po(\\theta_1+\\theta_2) \\] Kan approximeras som normalfördelning om µ är stort Multinomial 6. Slumptal Markovkedjor Stokastiska processer vars nästa värde endast beror på nuvarande värde. övergångsmatris används för att skriva upp “hoppsannolikheterna”. övergångssannolikheter av 2a ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv. alltså sannolikheten att mellanlanda i ett tillstånd. För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor \\[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \\] \\[ \\begin{pmatrix} 1 &amp; 2 \\end{pmatrix} * \\begin{pmatrix} 1to1 &amp; 1to2 \\ 2to1 &amp; 2to2 \\end{pmatrix} \\] terminologi beständigt tillstånd om P(i-&gt;i)=1 obeständigt tillstånd om P(i-&gt;i) less than 1 Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte. irreducibel om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också. stationär fördelning sannolikheterna att systemet befinner sig i de olika tillstånden. skapa sannolikhetsvektorn π = (π1,π2,..) lös ekv. π = πP (P är övergångsmatrisen) när \\[ p^{(n)}=(p_1^{(n)},p_2^{(n)},...) \\to \\pi \\] när \\[ n \\to \\infty \\] 1. om man i en ändlig kedja kan finna ett r&gt;0 så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en asymptotisk fördelning. 2. se stationär fördelning periodiska tillstånd om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3. aperiodiska tillstånd om det alltid går att komma tillbaka till ett tillstånd direkt DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial terminologi parameterrummet - de värden den sökta parametern kan tänkas anta. stickprov – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v. stickprovsvariansen \\[ s^2 = \\frac{1}{n-1} \\sum_{j=1}^n (x_j - \\bar{x})^2 \\] kovariansen mellan x- och y-värdena i en datamängd (x1,y1),(x2,y2),…,(xn,yn) \\[ c_{xy} = \\frac{1}{n-1}\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) \\] korrelationskoefficienten \\[ r = \\frac{c_{xy}}{s_xs_y} \\] 7. Punktskattning punktskattning – den observerade sannolikheten – ett utfall av stickprovsvariabeln \\[ \\theta_{obs}^*(x_1,x_2,…,x_n) \\] stickprovsvariabeln – en s.v. som punktskattningen är ett utfall av \\[ \\theta^*(X_1,X_2,…,X_n) \\] väntevärdesriktig – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \\[ E(\\theta^*) = \\theta \\] MSE – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \\[ MSE = E(( \\theta^* - \\theta)^2) \\] skattning av μ &amp; σ µ stickprovsmedelvärdet \\[ \\bar{x} \\] är en väntevärdesriktig och konsistent skattning av µ σ^2 stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2 Maximum-likelihood-metoden – ML-metoden Skapa \\[ L(\\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\\theta) \\] alt. \\[ L(\\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\\theta) \\] (A.k.a. likelihood-funktionen) Finn funktionens maxpunkt genom ex. derivering över theta. Funktionens största värde är det mest sannolika scenariot. Minsta-kvadrat-metoden – MK-metoden \\[ Q(\\theta) = \\sum_{i=1}^n [x_i - \\mu_i (\\theta)]^2 \\] Går ut på att anta att det finns små försöksfel vid varje mätdatum och bara genom att minimera dessa finner man bästa skattning av theta. Tillämpning på normalfördelningen Ett stickprov µ okänt σ känt \\[\\mu* = \\bar{x} \\] μ känt σ okänt \\[ (\\sigma^2)_{obs}^* = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\] Konfidensintervall för väntevärdet Känd standardavvikelse En lämplig skattning av µ är aritmetiska medelvärdet av X. \\[ \\bar{X} \\in N(\\mu,D) \\] \\[ D = \\sigma/\\sqrt{n} \\] \\[ I_\\mu = (\\bar{x}-\\lambda_{\\alpha/2}D,\\bar{x}+\\lambda_{\\alpha/2}D) \\] Allt detta följer av att: \\[ \\frac{\\bar{X}-\\mu}{D} \\in N(0,1) \\] Följaktligen gäller med sannolikheten 1-alfa att: \\[ -\\lambda_{\\alpha/2} &lt; \\frac{\\bar{X}-\\mu}{D} &lt; \\lambda_{\\alpha/2} &gt;\\] Om vi har ett intervall: \\[ I_\\mu = (16 \\pm 2.58 * 0.155) \\] där \\[ D = 1.2/\\sqrt{60} = 0.155 \\] och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation: \\[ 2 * 2.58 * 1.2/\\sqrt{n} = 0.5 \\] Okänd standardavvikelse I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ \\[ I_\\mu = (\\bar{x}-t_{\\alpha/2}(f)d,\\bar{x}+t_{\\alpha/2}(f)d) \\] \\[ d = s/\\sqrt{n}, \\quad f = n-1 \\] Konfidensintervall för standardavvikelsen μ känt Aint gonna happen gurl µ okänt \\[ I_\\sigma = (k_1s,k_2s) \\] \\[ k_1 = \\sqrt{(f/\\chi_{\\alpha/2}^2(f)} \\] \\[ k_2 = \\sqrt{(f/\\chi_{1-\\alpha/2}^2(f)} \\] \\[ f = n-1 \\] Två stickprov Om σ1 och σ2 är kända: \\[ I_{\\mu_1-\\mu_2} = (\\bar{x}-\\bar{y}-\\lambda_{\\alpha/2}D,\\bar{x}-\\bar{y}+\\lambda_{\\alpha/2}D) \\] \\[ D = \\sqrt{ \\sigma_{1}^{2} / n_1 + \\sigma_{2}^{2} / n_2} \\] Om σ1 = σ2 = σ: \\[ I_{\\mu_1-\\mu_2} = (\\bar{x}-\\bar{y}-t_{\\alpha/2}(f)d,\\bar{x}-\\bar{y}+t_{\\alpha/2}(f)d) \\] \\[ d = \\sigma \\sqrt{ \\frac{1}{n_1} + \\frac{1}{n_2}} \\] Stickprov i par Skapa \\[ z = y - x \\] 8. Intevallskattning När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval. Tillämpning på normalfördelningen Känd standardavvikelse 9. Hypotesprövning nollhypotes – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \\( H_0 \\) mothypotes – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \\( H_i \\) signifikansnivå/felrisk – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre). signifikant* – 0.05 signifikant** – 0.01 signifikant*** – 0.001 testvariabel/teststorhet – observation av stickprovsvariabel signifikanstest styrkefunktionen \\[ h(\\theta) = P(H_0 \\text{ förkastas}) \\] om θ är det rätta värdet bör vara stort för alla θ som tillhör mothypotesen bör vara litet för alla θ som tillhör nollhypotesen h(θ) kallas testets styrka för θ konfidensmetoden – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför 10. Regressionsanalys När man vill se samband mellan två eller flera storheter. Terminologi teoretiska regressionslinjen \\[ y = \\alpha + \\beta x \\] Punktskattningar Remember MK-metoden? Bestäm minimum för \\[ Q( \\alpha , \\beta ) = \\sum_{i}^n (y_i - \\mu_i)^2 \\] \\[ \\mu_i = \\alpha + \\beta x_i \\] Genom att sätta partialderivatorna till noll fås \\[ \\beta^* = \\frac{S_{xy}}{S_{xx}} \\quad \\alpha^* = \\bar{y} - \\beta^* \\bar{x} \\] Intervallskattningar 11. Fallgropar","@type":"BlogPosting","headline":"Sannolikhetsteori och statistikteori","dateModified":"2018-06-20T00:00:00+02:00","datePublished":"2018-06-20T00:00:00+02:00","url":"http://localhost:4000/2018/06/20/statistik.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/06/20/statistik.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="wlmr" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">wlmr</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/oenskelista.html">Önskelista</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Sannolikhetsteori och statistikteori</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-06-20T00:00:00+02:00" itemprop="datePublished">Jun 20, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>matematisk statistik</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">matematisk statistik</h1>
</header>
<h3 id="del-1-sannolikhet-eller-hur-man-beskriver-slumpen">DEL 1: Sannolikhet eller hur man beskriver slumpen</h3>
<p><em>utfall</em> – resultatet av ett slumpmässigt försök</p>
<p><em>utfallsrummet</em> – mängden möjliga utfall</p>
<p><em>händelse</em> – samling utfall</p>
<p><em>relativ frekvens</em> – kvoten mellan antalet erhållet utfall och hela antalet utförda kast</p>
<p><em>disjunkta händelser</em> – kan inte inträffa samtidigt</p>
<p><em>Kolmogorovs axiomsystem</em>:</p>
<ol type="1">
<li>Händelsen P(A) måste ligga mellan 0 &amp; 1</li>
<li>P(utfallsrummet) = 1</li>
<li>om A &amp; B är parvis oförenliga gäller \( P(A) + P(B) = P(A \cup B) \)</li>
</ol>
<p><em>komplementsatsen</em> P(A*) = 1 - P(A)</p>
<p><em>Additionssatsen</em> P(A or B) = P(A) + P(B) - P(A and B)</p>
<p><em>Booles olikhet</em> – <span class="math display">\[ P(A \cup B) \leq P(A) + P(B) \]</span></p>
<h5 id="kombinatorik">Kombinatorik</h5>
<p>Förutsättningar:</p>
<ul>
<li>n element</li>
<li>k av dessa plockas</li>
</ul>
<p><em>Klassiska sannolikhetsdefinitionen</em> Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall.</p>
<p><em>Dragning med återläggning och hänsyn till ordning</em> \[n^k\]</p>
<p><em>Dragning utan återläggning med hänsyn till ordning</em> \[n*(n-1)(n-2) \cdots (n-k)\]</p>
<p><em>Dragning utan återläggning utan hänsyn till ordning</em> \[\binom{n}{k}\]</p>
<p><em>Dragning utan återläggning</em> Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. <em>Klas. sann.</em> ges svaret av \[ g/m \] \[ m = \binom{v+s}{n} \] \[ g = \binom{v}{k} \binom{s}{n-k} \] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta.</p>
<p><em>Dragning med återläggning</em> Samma som ovan men med återläggning. \[ m = (v+s)^n \] \[ g = \binom{n}{k} v^k s^{n-k}\]</p>
<p>Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m.</p>
<p><em>Betingade sannolikheten</em> – sannolikheten att något inträffar givet en annan händelse.</p>
<p>\[P(B|A) = \frac{P(A \cap B)}{P(A)}\]</p>
<p>Ger alltså ett samband mellan betingning och snitt.</p>
<p><em>Lagen om total sannolikhet</em> \[P(A) = \sum_{i=1}^{n} P(H_i)P(A|H_i)\]</p>
<p><em>Bayes sats</em> \[P(H_i|A) = \frac{P(H_i)P(A|H_i)}{\sum_{j=1}^{n} P(H_j)P(A|H_j)}\]</p>
<p><em>Oberoende händelser</em> – P(B|A) = P(B)</p>
<p><em>sannolikheten att minst en inträffar</em> \[A_1 , A_2 , … , A_n är oberoende och P(A_i)=p_i\] \[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\]</p>
<h3 id="endimensionella-stokastiska-variabler">1. Endimensionella stokastiska variabler</h3>
<p>Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z.</p>
<h4 id="diskret-stokastisk-variabel">diskret stokastisk variabel</h4>
<p>En s.v. är <em>diskret</em> om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen.</p>
<ol type="1">
<li><p><em>Enpunktsfördelning</em> – all massa i ett värde \[p_X(a) = 1\]</p></li>
<li><em>Tvåpunktsfördelning</em> – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1-p.</li>
</ol>
<ul>
<li>ex: krona/klave</li>
<li>då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad.</li>
</ul>
<ol start="3" type="1">
<li><em>Likformig fördelning</em> – X antar värden 1,2,..,m och alla dessa med samma sannolikhet.</li>
</ol>
<ul>
<li>\[p_X(k)=1/m, k = 1,2,…,m.\]</li>
</ul>
<ol start="4" type="1">
<li><em>För-första-gången-fördelning</em></li>
</ol>
<ul>
<li>\[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\]</li>
<li>När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning.</li>
<li>\[ X \in ffg(p) \]</li>
</ul>
<ol start="5" type="1">
<li><em>Geometrisk fördelning</em> – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p)</li>
</ol>
<ul>
<li>\[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\]</li>
</ul>
<ol start="6" type="1">
<li><strong>Binomialfördelning</strong> – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr.</li>
</ol>
<ul>
<li>\[ p_X(k) = \binom{n}{k}p<sup>k(1-p)</sup>{n-k} \]</li>
<li>\[ X \in Bin(n,p) \]</li>
</ul>
<ol start="7" type="1">
<li><strong>Hypergeometrisk fördelning</strong> – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor.</li>
</ol>
<ul>
<li>\[ p_X(k) = \frac{\binom{v}{k} \binom{s}{n-k}}{\binom{v+s}{n}}\]</li>
<li>\[ X \in Hyp(N,n,p)\]</li>
</ul>
<ol start="8" type="1">
<li><strong>Poisson-fördelning</strong></li>
</ol>
<ul>
<li>\[ p_X(k) = \frac{\mu<sup>k}{k!}e</sup>{-e}\]</li>
<li>\[ X \in Po(\mu)\]</li>
</ul>
<h4 id="kontinuerlig-stokastisk-variabel">kontinuerlig stokastisk variabel</h4>
<p>Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f.</p>
<ol type="1">
<li><strong>Likformig fördelning</strong> – \[ f_X(x) = 1/(b-a) om a &lt; x &lt; b &gt;\]</li>
<li><strong>Exponentialfördelning</strong> – \[ f_X(x) = \lambda e^{-\lambda x} \]</li>
<li><strong>Normalfördelningen</strong></li>
<li><strong>Weibull-fördelning</strong></li>
<li><strong>Gammafördelning</strong></li>
</ol>
<p><strong>fördelningsfunktion</strong></p>
<p><strong>intensitet</strong></p>
<h3 id="flerdimensionella-stokastiska-variabler">2. Flerdimensionella stokastiska variabler</h3>
<h4 id="note-to-self">Note to self:</h4>
<ul>
<li>\[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \] OBS! Dont forget the last P(i)</li>
</ul>
<h4 id="största-och-minsta-värdet">Största och minsta värdet</h4>
<ul>
<li>Z = max(X,Y)
<ul>
<li>\[F_Z(z) = F_X(z)F_Y(z)\]</li>
</ul></li>
<li>Z = min(X,Y)
<ul>
<li>\[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\]</li>
</ul></li>
</ul>
<p>Gör först om till fördelningsfunktion om täthetsfunktion</p>
<h4 id="summan-av-s.v.">Summan av s.v.</h4>
<ul>
<li>\[f_Z(z) = \int_{\infty}^{\infty} f_X(x)f_Y(z-x)dx\]</li>
</ul>
<h3 id="väntevärden">3. Väntevärden</h3>
<h4 id="note-to-self-1">Note to self:</h4>
<p>konstanter inuti väntevärdesfunktioner är korkat.</p>
<h4 id="väntevärdet-ex-eller-μ">Väntevärdet E(X) eller μ</h4>
<p>Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade resultatet blir.</p>
<p>DEF: \[ E(X) = \sum_kkp_X(k) \] \[ E(X) = \int\limits_{-\infty}^{\infty} x f_X(x)dx \]</p>
<p>Y = g(X) \[ E(Y) = \sum_kg(k)p_X(k)\]</p>
<p>\[ E(X+Y) = E(X)+E(Y) \]</p>
<p>X &amp; Y oberoende \[ E(XY) = E(X)E(Y)\]</p>
<p>Samling X med samma väntevärde µ \[ E(\sum_{i=1}^nX_i)=n\mu \]</p>
<h5 id="betingade-väntevärden">Betingade väntevärden</h5>
<p>\[ E(X|Y=k) = \sum_{j=0}^\infty jp_{X|Y=k}(j)\]</p>
<p>\[ E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y=y}(x)dx\]</p>
<h4 id="variansen-vx-eller-σ">Variansen V(X) eller σ²</h4>
<ul>
<li>Variansen är en typ av spridningsmått.</li>
<li>DEF: \[ V(X) = E[(X-\mu)^2] \] – alltså ett väntevärde LOL</li>
<li><span class="math display">\[ V(X) = E(X^2)-[E(X)]^2 \]</span></li>
<li>\[ V(aX+b) = a^2V(X) \]</li>
<li>V(X + Y) = V(X) + V(Y) + 2C(X,Y)</li>
<li>Om oberoende: V(X + Y) = V(X) + V(Y)</li>
<li>Om oberoende och med samma σ: \[ V(\sum_{i=1}^nX_i) = n\sigma^2 \]</li>
<li>Om oberoende och med samma σ samt µ: \[ V(\bar{X})=\sigma^2/n \]</li>
</ul>
<h4 id="standardavvikelse-dx-eller-σ">Standardavvikelse D(X) eller σ</h4>
<ul>
<li>Schyst mått då man får samma dimension som väntevärdet</li>
<li>\[ D(X) = \sqrt{V(X)} \]</li>
<li>\[ D(aX + b) = |a|D(X) \]</li>
<li>Om oberoende: \[ D(X + Y) = \sqrt{D^2(X) + D^2(Y)} \]</li>
</ul>
<h4 id="variationskoefficienten">Variationskoefficienten</h4>
<p>uttrycks i procent \[ R(X) = D(X)/E(X) \]</p>
<h4 id="fel">fel</h4>
<ul>
<li><em>systematiskt fel/bias</em> är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal)</li>
<li><em>slumpmässigt fel</em> menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0)</li>
</ul>
<h4 id="beroendemått">Beroendemått</h4>
<h5 id="kovarians">Kovarians</h5>
<ul>
<li>Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden.</li>
<li>\( C(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \)</li>
<li>\( C(X,Y) = E(XY)-E(X)E(Y) \)</li>
<li>Om C(X,Y) = 0 är X och Y okorrelerade.</li>
<li>\( X \text{ &amp; } Y \text{ oberoende} \to \text{okorrelerade} \)</li>
</ul>
<h5 id="korrelationskoefficienten">Korrelationskoefficienten</h5>
<ul>
<li>DEF: \[ \rho(X,Y) = \frac{C(X,Y)}{D(X)D(Y)} \]</li>
<li>Kovarians fast dimensionslös</li>
</ul>
<h4 id="stora-talens-lag">Stora talens lag</h4>
<ul>
<li>Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ.</li>
</ul>
<h4 id="betingade-väntevärden-och-varianser">Betingade väntevärden och varianser</h4>
<h4 id="gauss-approximationsformler">Gauss approximationsformler</h4>
<p>Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen!</p>
<h5 id="en-variabel">En variabel</h5>
<ol type="1">
<li>taylorutveckla: \[ g(X) \approx g(\mu) + (X - \mu)g’(\mu) \]</li>
<li>g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ².</li>
</ol>
<h5 id="flera-variabler">Flera variabler</h5>
<h6 id="abandon-all-hope-ye-who-enter-here">Abandon all hope, ye who enter here</h6>
<ol type="1">
<li>taylorutveckla: \[ g(X,Y) \approx g(\mu_X,\mu_Y)+(X-\mu_X)g’_X(\mu_X,\mu_Y)+(Y-\mu_Y)g’_Y(\mu_X,\mu_Y) \]</li>
</ol>
<h3 id="normalfördelningen">4. Normalfördelningen</h3>
<h5 id="notes-to-self">Notes to self:</h5>
<ul>
<li>ca en tredjedel av massan hamnar utanför en standardavvikelse.</li>
<li>normalfördelningar bevaras alltid under linjära transformationer</li>
</ul>
<p><span class="math display">\[ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]</span></p>
<h4 id="standardiserad-fördelning">Standardiserad fördelning</h4>
<p>Täthetsfunktion: φ Fördelningsfunktion: Φ</p>
<p>\[ \Phi(-x) = 1 - \Phi(x) \]</p>
<h4 id="allmän-fördelning">Allmän fördelning</h4>
<p>\[ X \in N(\mu,\sigma) \quad iff \quad Y = (X-\mu)/\sigma \in N(0,1) \] \[ f_X(x) = \frac{1}{\sigma}\varphi(\frac{x-\mu}{\sigma}) \] \[ F_X(x) = \Phi(\frac{x-\mu}{\sigma}) \]</p>
<h4 id="linjärkombinationer">Linjärkombinationer</h4>
<p>Om \[ X \in N(\mu,\sigma) \] så gäller att \[ Y=aX+b \in N(a\mu + b, |a|\sigma) \]</p>
<p>Om \[ X_1,X_2,..,X_n \] är oberoende N(µ,σ) och \[ \bar{X} \] är medelvärdet så gäller att \[ \bar{X} \in N(\mu,\sigma/\sqrt{n}) \]</p>
<h3 id="de-tre-vännerna-och-binomialfördelning">5. De tre vännerna och Binomialfördelning</h3>
<h4 id="binomialaren-med-återläggning">binomialaren (med återläggning)</h4>
<p>E(X) = np V(X) = npq</p>
<p>Om oberoende \[ X \in Bin(n_1,p) \quad \&amp; Y \in Bin(n_2,p) \] \[ X + Y \in Bin(n_1+n_2,p) \]</p>
<p><strong>Obs! Glöm inte att bin är diskret, håll därför koll på gränserna (&gt; != &gt;=)</strong></p>
<p>Kan approximeras som</p>
<ol type="1">
<li><em>poissonfördelning</em> om p är litet</li>
<li><em>normalfördelning</em> om n är stort N(np,sqrt(npq))</li>
</ol>
<h4 id="hypergeometriske-utan-återläggning">Hypergeometriske (utan återläggning)</h4>
<p>E(X) = np V(X) = ((N-n)/(N-1))np(1-p)</p>
<p>Kan aproximeras som</p>
<ol type="1">
<li><em>binomialapproximation</em> om n/N är liten</li>
<li><em>normalapproximation</em> om n är stort</li>
</ol>
<h4 id="poisson-fördelningen">Poisson-fördelningen</h4>
<p>E(X) = µ V(X) = µ</p>
<p>\[ X_1 \in Po(\theta_1) \quad and \quad X_2 \in Po(\theta_2) \quad then \quad X_1+X_2 \in Po(\theta_1+\theta_2) \]</p>
<p>Kan approximeras som</p>
<ol type="1">
<li><em>normalfördelning</em> om µ är stort</li>
</ol>
<h4 id="multinomial">Multinomial</h4>
<h3 id="slumptal">6. Slumptal</h3>
<h3 id="markovkedjor">Markovkedjor</h3>
<p>Stokastiska processer vars nästa värde endast beror på nuvarande värde.</p>
<p>övergångsmatris används för att skriva upp “hoppsannolikheterna”.</p>
<p>övergångssannolikheter av 2a ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv. alltså sannolikheten att mellanlanda i ett tillstånd.</p>
<p>För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor <span class="math display">\[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \]</span></p>
<p>\[ \begin{pmatrix} 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} 1to1 &amp; 1to2 \ 2to1 &amp; 2to2 \end{pmatrix} \]</p>
<h5 id="terminologi">terminologi</h5>
<p><strong>beständigt</strong> tillstånd om P(i-&gt;i)=1 <strong>obeständigt</strong> tillstånd om P(i-&gt;i) less than 1</p>
<p><strong>Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte.</strong></p>
<p><strong>irreducibel</strong> om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också.</p>
<p><strong>stationär fördelning</strong> sannolikheterna att systemet befinner sig i de olika tillstånden.</p>
<ol type="1">
<li>skapa sannolikhetsvektorn π = (π1,π2,..)</li>
<li>lös ekv. π = πP (P är övergångsmatrisen)</li>
</ol>
<p>när <span class="math display">\[ p^{(n)}=(p_1^{(n)},p_2^{(n)},...) \to \pi \]</span> när \[ n \to \infty \] 1. om man i en ändlig kedja kan finna ett r&gt;0 så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en asymptotisk fördelning. 2. se stationär fördelning</p>
<p><em>periodiska tillstånd</em> om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3.</p>
<p><em>aperiodiska tillstånd</em> om det alltid går att komma tillbaka till ett tillstånd direkt</p>
<hr />
<h3 id="del-2-statistik-eller-vilka-slutsatser-man-kan-dra-av-ett-datamaterial">DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial</h3>
<h4 id="terminologi-1">terminologi</h4>
<p><em>parameterrummet</em> - de värden den sökta parametern kan tänkas anta.</p>
<p><em>stickprov</em> – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v.</p>
<p><em>stickprovsvariansen</em> \[ s^2 = \frac{1}{n-1} \sum_{j=1}^n (x_j - \bar{x})^2 \]</p>
<p><em>kovariansen mellan x- och y-värdena i en datamängd (x1,y1),(x2,y2),…,(xn,yn)</em> \[ c_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \] <em>korrelationskoefficienten</em> \[ r = \frac{c_{xy}}{s_xs_y} \]</p>
<h3 id="punktskattning">7. Punktskattning</h3>
<p><em>punktskattning</em> – den observerade sannolikheten – ett utfall av stickprovsvariabeln \[ \theta_{obs}^*(x_1,x_2,…,x_n) \]</p>
<p><em>stickprovsvariabeln</em> – en s.v. som punktskattningen är ett utfall av \[ \theta^*(X_1,X_2,…,X_n) \]</p>
<p><em>väntevärdesriktig</em> – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \[ E(\theta^*) = \theta \]</p>
<p><em>MSE</em> – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \[ MSE = E(( \theta^* - \theta)^2) \]</p>
<h4 id="skattning-av-μ-σ">skattning av μ &amp; σ</h4>
<h5 id="µ">µ</h5>
<p>stickprovsmedelvärdet \[ \bar{x} \] är en väntevärdesriktig och konsistent skattning av µ</p>
<h5 id="σ2">σ^2</h5>
<p>stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2</p>
<h4 id="maximum-likelihood-metoden-ml-metoden">Maximum-likelihood-metoden – ML-metoden</h4>
<ol type="1">
<li>Skapa \[ L(\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\theta) \] alt. \[ L(\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\theta) \] (A.k.a. likelihood-funktionen)</li>
<li>Finn funktionens maxpunkt genom ex. derivering över theta.</li>
<li>Funktionens största värde är det mest sannolika scenariot.</li>
</ol>
<h4 id="minsta-kvadrat-metoden-mk-metoden">Minsta-kvadrat-metoden – MK-metoden</h4>
<p>\[ Q(\theta) = \sum_{i=1}^n [x_i - \mu_i (\theta)]^2 \] Går ut på att anta att det finns små försöksfel vid varje mätdatum och bara genom att minimera dessa finner man bästa skattning av theta.</p>
<h4 id="tillämpning-på-normalfördelningen">Tillämpning på normalfördelningen</h4>
<h4 id="ett-stickprov">Ett stickprov</h4>
<h6 id="µ-okänt-σ-känt">µ okänt σ känt</h6>
<p>\[\mu* = \bar{x} \]</p>
<h6 id="μ-känt-σ-okänt">μ känt σ okänt</h6>
<p><span class="math display">\[ (\sigma^2)_{obs}^* = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \]</span></p>
<h5 id="konfidensintervall-för-väntevärdet">Konfidensintervall för väntevärdet</h5>
<h6 id="känd-standardavvikelse">Känd standardavvikelse</h6>
<p>En lämplig skattning av µ är aritmetiska medelvärdet av X. \[ \bar{X} \in N(\mu,D) \] \[ D = \sigma/\sqrt{n} \] \[ I_\mu = (\bar{x}-\lambda_{\alpha/2}D,\bar{x}+\lambda_{\alpha/2}D) \]</p>
<p>Allt detta följer av att:</p>
<p>\[ \frac{\bar{X}-\mu}{D} \in N(0,1) \]</p>
<p>Följaktligen gäller med sannolikheten 1-alfa att:</p>
<p>\[ -\lambda_{\alpha/2} &lt; \frac{\bar{X}-\mu}{D} &lt; \lambda_{\alpha/2} &gt;\]</p>
<p>Om vi har ett intervall:</p>
<p>\[ I_\mu = (16 \pm 2.58 * 0.155) \]</p>
<p>där</p>
<p>\[ D = 1.2/\sqrt{60} = 0.155 \]</p>
<p>och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation:</p>
<p>\[ 2 * 2.58 * 1.2/\sqrt{n} = 0.5 \]</p>
<h6 id="okänd-standardavvikelse">Okänd standardavvikelse</h6>
<p>I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ</p>
<p>\[ I_\mu = (\bar{x}-t_{\alpha/2}(f)d,\bar{x}+t_{\alpha/2}(f)d) \] \[ d = s/\sqrt{n}, \quad f = n-1 \]</p>
<h5 id="konfidensintervall-för-standardavvikelsen">Konfidensintervall för standardavvikelsen</h5>
<h6 id="μ-känt">μ känt</h6>
<p>Aint gonna happen gurl</p>
<h6 id="µ-okänt">µ okänt</h6>
<p>\[ I_\sigma = (k_1s,k_2s) \] \[ k_1 = \sqrt{(f/\chi_{\alpha/2}^2(f)} \] \[ k_2 = \sqrt{(f/\chi_{1-\alpha/2}^2(f)} \] \[ f = n-1 \]</p>
<h5 id="två-stickprov">Två stickprov</h5>
<p>Om σ1 och σ2 är kända:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-\lambda_{\alpha/2}D,\bar{x}-\bar{y}+\lambda_{\alpha/2}D) \] \[ D = \sqrt{ \sigma_{1}^{2} / n_1 + \sigma_{2}^{2} / n_2} \]</p>
<p>Om σ1 = σ2 = σ:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-t_{\alpha/2}(f)d,\bar{x}-\bar{y}+t_{\alpha/2}(f)d) \] \[ d = \sigma \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}} \]</p>
<h5 id="stickprov-i-par">Stickprov i par</h5>
<p>Skapa \[ z = y - x \]</p>
<h3 id="intevallskattning">8. Intevallskattning</h3>
<p>När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval.</p>
<h4 id="tillämpning-på-normalfördelningen-1">Tillämpning på normalfördelningen</h4>
<h5 id="känd-standardavvikelse-1">Känd standardavvikelse</h5>
<h3 id="hypotesprövning">9. Hypotesprövning</h3>
<p><strong>nollhypotes</strong> – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \( H_0 \)</p>
<p><strong>mothypotes</strong> – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \( H_i \)</p>
<p><strong>signifikansnivå/felrisk</strong> – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre).</p>
<ul>
<li>signifikant* – 0.05</li>
<li>signifikant** – 0.01</li>
<li>signifikant*** – 0.001</li>
</ul>
<p><strong>testvariabel/teststorhet</strong> – observation av stickprovsvariabel</p>
<p><strong>signifikanstest</strong></p>
<p><strong>styrkefunktionen</strong> \[ h(\theta) = P(H_0 \text{ förkastas}) \] om θ är det rätta värdet</p>
<ul>
<li>bör vara stort för alla θ som tillhör mothypotesen</li>
<li>bör vara litet för alla θ som tillhör nollhypotesen</li>
<li>h(θ) kallas testets styrka för θ</li>
</ul>
<p><strong>konfidensmetoden</strong> – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför</p>
<h3 id="regressionsanalys">10. Regressionsanalys</h3>
<p>När man vill se samband mellan två eller flera storheter.</p>
<h4 id="terminologi-2">Terminologi</h4>
<p><strong>teoretiska regressionslinjen</strong> \[ y = \alpha + \beta x \]</p>
<h4 id="punktskattningar">Punktskattningar</h4>
<p>Remember MK-metoden? Bestäm minimum för</p>
<p>\[ Q( \alpha , \beta ) = \sum_{i}^n (y_i - \mu_i)^2 \] \[ \mu_i = \alpha + \beta x_i \]</p>
<p>Genom att sätta partialderivatorna till noll fås</p>
<p>\[ \beta^* = \frac{S_{xy}}{S_{xx}} \quad \alpha^* = \bar{y} - \beta^* \bar{x} \]</p>
<h4 id="intervallskattningar">Intervallskattningar</h4>
<h3 id="fallgropar">11. Fallgropar</h3>
</body>
</html>

  </div><a class="u-url" href="/2018/06/20/statistik.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">wlmr</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">wlmr</li><li><a class="u-email" href="mailto:wlmr@pm.me">wlmr@pm.me</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/wlmr"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">wlmr</span></a></li><li><a href="https://www.twitter.com/wlmrn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">wlmrn</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is where I will keep useful info and share my opinions.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
