---
layout: post
title: "Sannolikhetsteori och statistikteori"
---
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2018-08-13-Sannolikhetsteori_och_statistikteori</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="del-1-sannolikhet-eller-hur-man-beskriver-slumpen">DEL 1: Sannolikhet eller hur man beskriver slumpen</h2>
<p><strong>utfall</strong> – resultatet av ett slumpmässigt försök</p>
<p><strong>utfallsrummet</strong> – mängden möjliga utfall</p>
<p><strong>händelse</strong> – samling utfall</p>
<p><strong>relativ frekvens</strong> – kvoten mellan antalet erhållet utfall och hela antalet utförda kast</p>
<p><strong>disjunkta händelser</strong> – kan inte inträffa samtidigt</p>
<p><strong>Kolmogorovs axiomsystem</strong></p>
<ol type="1">
<li>Händelsen P(A) måste ligga mellan 0 &amp; 1</li>
<li>P(utfallsrummet) = 1</li>
<li>om A &amp; B är parvis oförenliga gäller \( P(A) + P(B) = P(A \cup B) \)</li>
</ol>
<p><strong>komplementsatsen</strong> P(A*) = 1 - P(A)</p>
<p><strong>Additionssatsen</strong> P(A or B) = P(A) + P(B) - P(A and B)</p>
<p><strong>oberoende händelser</strong> – \( P(A \cap B) = P(A)P(B) \)</p>
<p><strong>Booles olikhet</strong> – \( P(A \cup B) \leq P(A) + P(B) \)</p>
<p><strong>De morgans lagar</strong> – bra att veta att även boolsk algebra är distributiv.</p>
<p>\[ A \cup (B \cap C) = (A \cup B) \cap (A \cup C) \] \[ A \cap (B \cup C) = (A \cap B) \cup (A \cap C) \]</p>
<h4 id="kombinatorik">Kombinatorik</h4>
<p>Förutsättningar:</p>
<ul>
<li>n element</li>
<li>k av dessa plockas</li>
</ul>
<p><strong>Klassiska sannolikhetsdefinitionen</strong> Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall.</p>
<p><strong>Dragning med återläggning och hänsyn till ordning</strong> \[n^k\]</p>
<p><strong>Dragning utan återläggning med hänsyn till ordning</strong> \[n*(n-1)(n-2) \cdots (n-k)\]</p>
<p><strong>Dragning utan återläggning utan hänsyn till ordning</strong> \[\binom{n}{k}\]</p>
<p><strong>Dragning utan återläggning</strong> Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. __Klas. sann._ ges svaret av \[ g/m \] \[ m = \binom{v+s}{n} \] \[ g = \binom{v}{k} \binom{s}{n-k} \] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta.</p>
<p><strong>Dragning med återläggning</strong> Samma som ovan men med återläggning. \[ m = (v+s)^n \] \[ g = \binom{n}{k} v^k s^{n-k}\]</p>
<p>Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m.</p>
<p><strong>Betingade sannolikheten</strong> – sannolikheten att något inträffar givet en annan händelse.</p>
<p>\[P(B|A) = \frac{P(A \cap B)}{P(A)}\]</p>
<p>Ger alltså ett samband mellan betingning och snitt.</p>
<p><strong>Lagen om total sannolikhet</strong> – genom att summera de olika produkter som ges av sannolikheten för varje möjligt utfall \( H_i \) multiplicerat med sannolikheten att A händer om vi faktiskt fått \( H_i \) får vi den totala sannolikheten att A händer.</p>
<p>\[P(A) = \sum_{i=1}^{n} P(H_i)P(A|H_i)\]</p>
<p><strong>Bayes sats</strong> – när man behöver vända på en betingad sannolikhet. Nämnaren i bråket är alltså högerledet i lagen om total sannolikhet.</p>
<p>\[P(H_i|A) = \frac{P(H_i)P(A|H_i)}{\sum_{j=1}^{n} P(H_j)P(A|H_j)}\]</p>
<p><strong>Oberoende händelser</strong> – \( P(B|A) = P(B) \)</p>
<p><strong>sannolikheten att minst en inträffar</strong> \[A_1 , A_2 , … , A_n \text{ är oberoende }, \quad P(A_i)=p_i\] \[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\]</p>
<h3 id="endimensionella-stokastiska-variabler">1. Endimensionella stokastiska variabler</h3>
<p>Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z.</p>
<h4 id="diskret-stokastisk-variabel">diskret stokastisk variabel</h4>
<p>En s.v. är __diskret_ om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen.</p>
<ol type="1">
<li><p><strong>Enpunktsfördelning</strong> – all massa i ett värde \[p_X(a) = 1\]</p></li>
<li><p><strong>Tvåpunktsfördelning</strong> – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1- p. ex: krona/klave då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad.</p></li>
<li><p><strong>Likformig fördelning</strong> – X antar värden 1,2,..,m och alla dessa med samma sannolikhet. \[p_X(k)=1/m, k = 1,2,…,m.\]</p></li>
<li><p><strong>För-första-gången-fördelning</strong> – När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning. \[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\] \[ X \in ffg(p) \]</p></li>
<li><p><strong>Geometrisk fördelning</strong> – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p). \[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\]</p></li>
<li><p><strong>Binomialfördelning</strong> – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr. \[ p_X(k) = \binom{ n }{ k } p^k ( 1 - p )^{ n - k } \] \[ X \in Bin(n,p) \]</p></li>
<li><p><strong>Hypergeometrisk fördelning</strong> – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor. \[ p_X(k) = \frac{\binom{v}{k} \binom{s}{n-k}}{\binom{v+s}{n}}\] \[ X \in Hyp(N,n,p)\]</p></li>
<li><p><strong>Poisson-fördelning</strong> – beskriver antalet företelser som inträffar oberoende av varandra. \[ p_X(k) = \frac{ \mu^k }{ k! }e^{ -e } \] \[ X \in Po(\mu)\]</p></li>
</ol>
<h4 id="kontinuerlig-stokastisk-variabel">kontinuerlig stokastisk variabel</h4>
<p>Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f.</p>
<ol type="1">
<li><p><strong>Likformig fördelning</strong> – X antar alla värden mellan a och b med samma sannolikhet \[ f_X(x) = 1/(b-a) \text{ om } a &lt; x &lt; b &gt;\]</p></li>
<li><p><strong>Exponentialfördelning</strong> – beskriver tiderna mellan händelserna i en poissonprocess. \[ f_X(x) = \lambda e^{-\lambda x} \quad E(X) = 1 / \lambda, \quad D(X) = 1 / \lambda \]</p></li>
<li><p><strong>Normalfördelningen</strong></p></li>
<li><p><strong>Weibull-fördelning</strong></p></li>
<li><p><strong>Gammafördelning</strong></p></li>
</ol>
<p><strong>fördelningsfunktion</strong> – Funktion som för varje möjligt utfall beräknar sannolikheten att utfallet blir just detta eller lägre.</p>
<p><strong>funktioner av stokastisk variabel</strong> – När man vill veta hur en s.v., Y som beror av g(x), ser ut. Nedan följer generellt tillvägagångssätt för att beskriva \( f_y (y) \) i termer av den kända \( f_x (y) \). Med exemplet: \( g(x) = |X| \)</p>
<ol type="1">
<li>Beskriv: \( F_y (y) = P( Y \leq y) \)</li>
<li>Byt ut Y mot g(X): \( P( g(X) \leq y) \)</li>
<li>Lös ut X: \( P( -y \leq X \leq y) = F_X (y) - F_X (-y) \)</li>
<li>\(f_Y (y) = f_X (y) + f_X (-y) \)</li>
</ol>
<p><strong>intensitet</strong></p>
<h3 id="flerdimensionella-stokastiska-variabler">2. Flerdimensionella stokastiska variabler</h3>
<h4 id="note-to-self">Note to self:</h4>
<ul>
<li>\[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \] OBS! Dont forget the last P(i)</li>
</ul>
<h4 id="största-och-minsta-värdet">Största och minsta värdet</h4>
<ul>
<li><p>Z = max(X,Y) \[F_Z(z) = F_X(z)F_Y(z)\]</p></li>
<li><p>Z = min(X,Y) \[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\]</p></li>
</ul>
<p>Gör först om till fördelningsfunktion om täthetsfunktion</p>
<h4 id="summan-av-s.v.">Summan av s.v.</h4>
<ul>
<li>\[f_Z(z) = \int_{\infty}^{\infty} f_X(x)f_Y(z-x)dx\]</li>
</ul>
<h3 id="väntevärden">3. Väntevärden</h3>
<p><strong>Note to self:</strong> konstanter inuti väntevärdesfunktioner ser dumt ut så flytta ut dem.</p>
<h4 id="väntevärdetexμ">Väntevärdet/E(X)/μ</h4>
<p>Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade (__E__xpected) resultatet blir.</p>
<p><strong>DEF:</strong> \[ E(X) = \sum_kkp_X(k) \] \[ E(X) = \int\limits_{-\infty}^{\infty} x f_X(x)dx \]</p>
<p><strong>Y = g(X)</strong> – Väljer du att interfacea funktionen med en ny variabel som beror av X med samma gamla fördelning så kan man trixa enl. följande \[ E(Y) = \sum_kg(k)p_X(k)\] \[ E(X) = \int\limits_{-\infty}^{\infty} g(x) f_X(x)dx \]</p>
<p>\[ E(X+Y) = E(X)+E(Y) \]</p>
<p><strong>X &amp; Y oberoende</strong></p>
<p>\[ E(XY) = E(X)E(Y)\]</p>
<p>Samling X med samma väntevärde µ \[ E(\sum_{i=1}^n X_i)=n\mu \]</p>
<p><strong>Betingade väntevärden</strong></p>
<p>\[ E(X|Y=k) = \sum_{j=0}^\infty jp_{X|Y=k}(j)\]</p>
<p>\[ E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y=y}(x)dx\]</p>
<h4 id="variansenvxσ">Variansen/V(X)/σ²</h4>
<p>Variansen är en typ av spridningsmått. Vid beräkning av varians av en summa olika s.v. bör man <strong>ALDRIG</strong> anta att inblandade s.v. är oberoende. GLÖM MED ANDRA ORD INTE KOVARIANSEN.</p>
<p><strong>DEF:</strong> \[ V(X) = E[(X-\mu)^2] \]</p>
<p><strong>Schysta satser:</strong> <span class="math display">\[ V(X) = E(X^2)-[E(X)]^2 \]</span> \[ V(aX+b) = a^2V(X) \] \[ V(X + Y) = V(X) + V(Y) + 2C(X,Y) \]</p>
<ul>
<li>Om oberoende: \( V(X + Y) = V(X) + V(Y) \)</li>
<li>Om oberoende och med samma σ: \( V(\sum_{i=1}^nX_i) = n\sigma^2 \)</li>
<li>Om oberoende och med samma σ samt µ: \( V(\bar{X})=\sigma^2/n \)</li>
</ul>
<p>Tänk på att använda formelsamlingen för att snabbt fastställa variansen för de olika fördelningarna.</p>
<h4 id="standardavvikelse-dx-eller-σ">Standardavvikelse D(X) eller σ</h4>
<p>Schyst mått då man får samma dimension som väntevärdet \[ D(X) = \sqrt{V(X)} \] \[ D(aX + b) = |a|D(X) \] Om oberoende: \[ D(X + Y) = \sqrt{D^2(X) + D^2(Y)} \]</p>
<h4 id="variationskoefficienten">Variationskoefficienten</h4>
<p>uttrycks i procent \[ R(X) = D(X)/E(X) \]</p>
<h4 id="fel">fel</h4>
<ul>
<li><strong>systematiskt fel/bias</strong> är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal)</li>
<li><strong>slumpmässigt fel</strong> menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0)</li>
</ul>
<h4 id="beroendemått">Beroendemått</h4>
<h5 id="kovarians">Kovarians</h5>
<ul>
<li>Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden.</li>
<li>\( C(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \)</li>
<li>\( C(X,Y) = E(XY)-E(X)E(Y) \)</li>
<li>Om C(X,Y) = 0 är X och Y okorrelerade.</li>
<li>\( X \text{ &amp; } Y \text{ oberoende} \to \text{okorrelerade} \)</li>
</ul>
<h5 id="korrelationskoefficienten">Korrelationskoefficienten</h5>
<ul>
<li>DEF: \[ \rho(X,Y) = \frac{C(X,Y)}{D(X)D(Y)} \]</li>
<li>Kovarians fast dimensionslös</li>
</ul>
<h4 id="stora-talens-lag">Stora talens lag</h4>
<ul>
<li>Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ.</li>
</ul>
<h4 id="betingade-väntevärden-och-varianser">Betingade väntevärden och varianser</h4>
<h4 id="gauss-approximationsformler">Gauss approximationsformler</h4>
<p>Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen!</p>
<h5 id="en-variabel">En variabel</h5>
<ol type="1">
<li>taylorutveckla: \[ g(X) \approx g(\mu) + (X - \mu)g’(\mu) \]</li>
<li>g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ².</li>
</ol>
<h5 id="flera-variabler">Flera variabler</h5>
<h6 id="abandon-all-hope-ye-who-enter-here">Abandon all hope, ye who enter here</h6>
<ol type="1">
<li>taylorutveckla: \[ g(X,Y) \approx g(\mu_X,\mu_Y)+(X-\mu_X)g’_X(\mu_X,\mu_Y)+(Y-\mu_Y)g’_Y(\mu_X,\mu_Y) \]</li>
</ol>
<h3 id="normalfördelningen">4. Normalfördelningen</h3>
<h5 id="notes-to-self">Notes to self:</h5>
<ul>
<li>ca en tredjedel av massan hamnar utanför en standardavvikelse.</li>
<li>normalfördelningar bevaras alltid under linjära transformationer</li>
</ul>
<p><span class="math display">\[ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]</span></p>
<h4 id="standardiserad-fördelning">Standardiserad fördelning</h4>
<p>Täthetsfunktion: φ Fördelningsfunktion: Φ</p>
<p>\[ \Phi(-x) = 1 - \Phi(x) \]</p>
<h4 id="allmän-fördelning">Allmän fördelning</h4>
<p>\[ X \in N(\mu,\sigma) \quad iff \quad Y = (X-\mu)/\sigma \in N(0,1) \] \[ f_X(x) = \frac{1}{\sigma}\varphi(\frac{x-\mu}{\sigma}) \] \[ F_X(x) = \Phi(\frac{x-\mu}{\sigma}) \]</p>
<h4 id="linjärkombinationer">Linjärkombinationer</h4>
<p>Om \[ X \in N(\mu,\sigma) \] så gäller att \[ Y=aX+b \in N(a\mu + b, |a|\sigma) \]</p>
<p>Om \[ X_1,X_2,..,X_n \] är oberoende N(µ,σ) och \[ \bar{X} \] är medelvärdet så gäller att \[ \bar{X} \in N(\mu,\sigma/\sqrt{n}) \]</p>
<h3 id="de-tre-vännerna-och-binomialfördelning">5. De tre vännerna och Binomialfördelning</h3>
<h4 id="binomialaren-fördelningen-med-återläggning">binomialaren (fördelningen med återläggning)</h4>
<p>E(X) = np V(X) = npq</p>
<p>Om oberoende \[ X \in Bin(n_1,p) \quad \&amp; Y \in Bin(n_2,p) \] \[ X + Y \in Bin(n_1+n_2,p) \]</p>
<p><strong>Obs! Glöm inte att Binomialfördelningen är diskret, håll därför koll på gränserna (&gt; != &gt;=)</strong></p>
<p>Kan approximeras till</p>
<p><strong>poissonfördelning</strong> – om p är litet <strong>normalfördelning</strong> – om npq &gt; 10 kan man approximera med N(np,sqrt(npq)). Resultatet blir noggrannare om man vid beräkning av fördelningsfunktion adderar 1/2 till gränsen/gränserna.</p>
<p>Skillnad mellan två binomialfördelade s.v. approximeras till \[ Y_1 - Y_2 \in N(p_1 - p_2, \sqrt{ \frac{p_1 (1-p_2)}{n_1} + \frac{ p_2 (1- p_1)}{n_2}} ) \]</p>
<h4 id="hypergeometriske-utan-återläggning">Hypergeometriske (utan återläggning)</h4>
<p>E(X) = np V(X) = ((N-n)/(N-1))np(1-p)</p>
<p>Kan aproximeras som</p>
<ol type="1">
<li><strong>binomialapproximation</strong> om n/N är liten</li>
<li><strong>normalapproximation</strong> om n är stort</li>
</ol>
<h4 id="poisson-fördelningen">Poisson-fördelningen</h4>
<p>E(X) = µ V(X) = µ</p>
<p>\[ X_1 \in Po( \theta_1 ) \quad and \quad X_2 \in Po( \theta_2 ) \quad then \quad X_1+X_2 \in Po(\theta_1+\theta_2) \]</p>
<p>Kan approximeras som</p>
<ol type="1">
<li><strong>normalfördelning</strong> om µ är stort</li>
</ol>
<h4 id="multinomial">Multinomial</h4>
<h3 id="markovkedjor">Markovkedjor</h3>
<ul>
<li><p>Stokastiska processer vars nästa värde endast beror på nuvarande värde.</p></li>
<li><p>övergångsmatris används för att skriva upp “hoppsannolikheterna”.</p></li>
<li><p>övergångssannolikheter av N:te ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv n ggr. alltså sannolikheten att mellanlanda i ett tillstånd.</p></li>
</ul>
<p>För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor <span class="math display">\[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \]</span></p>
<p>\[ \begin{pmatrix} 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} 1to1 &amp; 1to2 \\ 2to1 &amp; 2to2 \end{pmatrix} \]</p>
<h5 id="terminologi">terminologi</h5>
<p><strong>tillstånd</strong> – markovkedja är alltid i ett tillstånd. Den lämnar detta enl. övergångsmatrisen</p>
<p><strong>ändlig</strong> – ändligt antal tillstånd i kedjan</p>
<p><strong>beständigt</strong> – om tillstånd där \( p_{ii} = 1 \) (Gäller alla olika ordningars övergångsmatriser)</p>
<ol type="1">
<li>kolla första ordningens sannolikhet att kedjan väljer att hoppa till tillståndet det redan var i, sedan andra ordningens och tredje ordningens. Fortsätt tills du hittat ett samband mellan sannolikheterna, ex. i form av en geometrisk serie. Blir summan 1 har du ett <strong>beständigt tillstånd</strong>.</li>
<li>Använd sedan faktumet att om <strong>två tillstånd kommunicerar tvåsidigt är de båda antinge beständiga eller båda inte</strong></li>
<li>Använd sedan faktumet att om alla \( p_{ij} = 0 \) så är j inte beständigt (går ju inte att komma dit (om man inte börjar där men inte sedan kan man ändå inte komma dit igen)).</li>
</ol>
<p><strong>obeständigt</strong> – tillstånd om P(i-&gt;i) less than 1</p>
<p><strong>Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte.</strong></p>
<p><strong>irreducibel</strong> – om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också.</p>
<ol type="1">
<li>rita upp markovkedjan och följ pilarna</li>
</ol>
<p><strong>stationär fördelning</strong> – sannolikheterna att systemet befinner sig i de olika tillstånden.</p>
<ol type="1">
<li>skapa sannolikhetsvektorn \( \pi = ( \pi_1, \pi_2,..) \)</li>
<li>lös ekv. \( \pi = \pi P \) (P är övergångsmatrisen)</li>
</ol>
<p>\[ p^{(n)}=( p_1^{(n)}, p_2^{(n)} ,…) \to \pi, \quad \text{ när } n \to \infty \]</p>
<p><strong>asymptotisk fördelning</strong> – om man i en ändlig kedja kan finna ett \( r&gt;0 \) så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en <strong>asymptotisk</strong> fördelning</p>
<p><strong>periodiska tillstånd</strong> – om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3.</p>
<p><strong>aperiodiska tillstånd</strong> – om det alltid går att komma tillbaka till ett tillstånd direkt..</p>
<h2 id="del-2-statistik-eller-vilka-slutsatser-man-kan-dra-av-ett-datamaterial">DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial</h2>
<h3 id="terminologi-1">terminologi</h3>
<p><strong>parameterrummet</strong> – de värden den sökta parametern kan tänkas anta.</p>
<p><strong>stickprov</strong> – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v.</p>
<p><strong>stickprovsvariansen</strong> – \( s^2 = \frac{1}{n-1} \sum_{j=1}^n (x_j - \bar{x})^2 \)</p>
<p><strong>kovariansen</strong> – mellan x- och y-värdena i en datamängd \( ( x_1 , y_1 ),( x_2 , y_2 ),…,( x_n , y_n ) \) \[ c_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \]</p>
<p><strong>korrelationskoefficienten</strong> – \( r = \frac{c_{xy}}{s_xs_y} \)</p>
<h3 id="punktskattning">7. Punktskattning</h3>
<p>Lär dig uppskatta saker!</p>
<p><strong>punktskattning</strong> – den observerade sannolikheten – ett utfall av stickprovsvariabeln \[ \theta_{obs}^*(x_1,x_2,…,x_n) \]</p>
<p><strong>stickprovsvariabeln</strong> – en s.v. som punktskattningen är ett utfall av \[ \theta^*(X_1,X_2,…,X_n) \]</p>
<p><strong>väntevärdesriktig</strong> – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \[ E(\theta^*) = \theta \]</p>
<p><strong>MSE</strong> – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \[ MSE = E(( \theta^* - \theta)^2) \]</p>
<p><strong>effektiv skattning</strong> – om variansen för \( V( \theta^* ) \leq V( \hat{\theta} ) \) så är \( \theta^* \) effektivare än den andra.</p>
<h4 id="skattning-av-μ-σ">Skattning av μ &amp; σ</h4>
<p><strong>µ</strong> – stickprovsmedelvärdet \[ \bar{x} \] är en väntevärdesriktig och konsistent skattning av µ och gäller för alla fördelningar</p>
<p><strong>σ^2</strong> – stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2 som gäller för alla fördelningar och ser ut så här om μ är känd \[ \frac{ 1 }{ n } \sum_i^n (x_i - \mu)^2 \] Annars skattas μ med medelvärdet av mätdata och man får efter lite väntevärdesriktighetsjustering \[ \frac{ 1 }{ n - 1 } \sum_i^n (x_i - \bar{x})^2 \]</p>
<h4 id="skattningsfunktioner">Skattningsfunktioner</h4>
<p><strong>Maximum-likelihood-metoden – ML-metoden</strong> – Om man vill skatta en parameter till en täthets/sannolikhetsfunktion. Genom att sätta in varje mätdata i sin sannolikhetsfunktion och sedan multiplicera alla dessa får vi sannolikheten att utfallen blev just mätdatan. Vi utnyttjar nu att sannolikhetsfunktionen är som störst när avståndet mellan väntevärdet och mätdatan är som minst genom att derivera över den okända parametern och söka maxpunkt.</p>
<ol type="1">
<li>Skapa \[ L(\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\theta) \] alt. \[ L(\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\theta) \] (A.k.a. likelihood-funktionen)</li>
<li>Finn funktionens maxpunkt genom ex. derivering över theta.</li>
<li>Funktionens största värde är det mest sannolika scenariot.</li>
</ol>
<p><strong>Minsta-kvadrat-metoden – MK-metoden</strong> – Om man vill skatta en parameter till en täthets/sannolikhetsfunktion. När man vet hur parametern förhåller sig till mätdatan kan man ställa upp en summa där varje term motsvarar \( (x_i - g( \theta ) )^2 \). Genom att finna derivatans minimum får vi reda på det värde på θ som gör differanserna så små som möjligt. Så hur går man då tillväga? Jo genom att skapa funktionen \( Q( \theta )) \) och sedan derivera denna och finna minimum för</p>
<p>\[ Q(\theta) = \sum_{i=1}^n [x_i - \mu_i (\theta)]^2 \] \[ \frac{dQ}{d \theta} Q( \theta ) \]</p>
<ul>
<li><p>Viss mätdata sämre än resten? Använd vikter framför termerna i \( Q ( \theta ) \) så att de sämre mätningarna får mindre betydelse.</p></li>
<li><p>Flera parametrar? Lös partialderivatorna och sedan ekvationssystemet.</p></li>
</ul>
<h4 id="normalfördelningen-1">Normalfördelningen</h4>
<p><strong>Två stickprov med samma σ</strong> – M.h.a. ML-skattning får vi som väntat \( ( \mu_1 )<em>{obs}^* = \bar{x} \) samt \( \mu_2 )</em>{obs}^* = \bar{y} \). variansen, å andra sidan, skattas av \[ s^2 = \frac{ \sum_{i_1}^{n_1} (x_i - \bar{x} )^2 +\sum_{i_1}^{n_2} (y_i - \bar{y} )^2 }{ (n_1 - 1 ) + (n_2 - 1 ) } \]</p>
<p><strong>n stickprov med samma σ</strong> – En skattning som gäller oavsett hur många stickprov man blandar in ges av \[ s^2 = \frac{ Q_1 + … + Q_n }{ (n_1 - 1) + … + (n_2 - 1) } \] \( Q_i \) är kvadratsumman kring medelvärdet.</p>
<h3 id="intevallskattning">8. Intevallskattning</h3>
<p>När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval.</p>
<h4 id="tillämpning-på-normalfördelningen">Tillämpning på normalfördelningen</h4>
<h4 id="ett-stickprov">Ett stickprov</h4>
<p><strong>µ okänd σ känd</strong></p>
<p>\[\mu* = \bar{x} \]</p>
<p><strong>μ känd σ okänd</strong></p>
<p><span class="math display">\[ (\sigma^2)_{obs}^* = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \]</span></p>
<h5 id="konfidensintervall-för-väntevärdet">Konfidensintervall för väntevärdet</h5>
<p><strong>σ känd</strong></p>
<p>En lämplig skattning av µ är aritmetiska medelvärdet av X.</p>
<p>\[ \bar{X} \in N(\mu,D) \] \[ D = \sigma/\sqrt{n} \] \[ I_\mu = (\bar{x}-\lambda_{\alpha/2}D,\bar{x}+\lambda_{\alpha/2}D) \]</p>
<p>Allt detta följer av att:</p>
<p>\[ \frac{\bar{X}-\mu}{D} \in N(0,1) \]</p>
<p>Följaktligen gäller med sannolikheten 1-alfa att:</p>
<p>\[ -\lambda_{\alpha/2} &lt; \frac{\bar{X}-\mu}{D} &lt; \lambda_{\alpha/2} &gt;\]</p>
<p>Om vi har ett intervall:</p>
<p>\[ I_\mu = (16 \pm 2.58 * 0.155) \]</p>
<p>där</p>
<p>\[ D = 1.2/\sqrt{60} = 0.155 \]</p>
<p>och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation:</p>
<p>\[ 2 * 2.58 * 1.2/\sqrt{n} = 0.5 \]</p>
<p><strong>σ okänd</strong></p>
<p>I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ</p>
<p>\[ I_\mu = (\bar{x}-t_{\alpha/2}(f)d,\bar{x}+t_{\alpha/2}(f)d) \] \[ d = s/\sqrt{n}, \quad f = n-1 \]</p>
<h5 id="konfidensintervall-för-standardavvikelsen">Konfidensintervall för standardavvikelsen</h5>
<p><strong>μ känd</strong></p>
<p>Aint gonna happen gurl</p>
<p><strong>μ okänd</strong></p>
<p>\[ I_\sigma = (k_1s,k_2s) \] \[ k_1 = \sqrt{(f/\chi_{\alpha/2}^2(f)} \] \[ k_2 = \sqrt{(f/\chi_{1-\alpha/2}^2(f)} \] \[ f = n-1 \]</p>
<h4 id="två-stickprov">Två stickprov</h4>
<p>När man vill mäta skillnaden mellan två stickprov. Om de två stickproven parvis korrelerar bör metoden tillhörande stickprov i par användas.</p>
<p>Om σ1 och σ2 är kända:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-\lambda_{\alpha/2}D,\bar{x}-\bar{y}+\lambda_{\alpha/2}D) \] \[ D = \sqrt{ \sigma_{1}^{2} / n_1 + \sigma_{2}^{2} / n_2} \]</p>
<p>Om σ1 = σ2 = σ:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-t_{\alpha/2}(f)d,\bar{x}-\bar{y}+t_{\alpha/2}(f)d) \] \[ d = \sigma \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}} \]</p>
<h4 id="stickprov-i-par">Stickprov i par</h4>
<p>När det utförs två mätningar på n olika objekt, t.ex. före och efter något har hänt. Då alla objekt kan skilja sig åt är det svårt att göra skattningar om de enskilda objekten men på samma gång perfekt läge för att skatta skillnaden mellan mätningarna.</p>
<p>Skapa \( z = y - x \) och använd z för skattning av standardavvikelse.</p>
<h3 id="hypotesprövning">9. Hypotesprövning</h3>
<p><strong>nollhypotes</strong> – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \( H_0 \)</p>
<p><strong>mothypotes</strong> – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \( H_i \)</p>
<p><strong>signifikansnivå/felrisk</strong> – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre).</p>
<ul>
<li>signifikant* – 0.05</li>
<li>signifikant** – 0.01</li>
<li>signifikant*** – 0.001</li>
</ul>
<p><strong>testvariabel/teststorhet</strong> – observation av stickprovsvariabel. Tackvare normalapproximation lämpar sig ofta testvariabler på formen \( ( \theta^* - \theta_0 )/D \). Där \( \theta^* \) är det skattade väntevärdet, \( \theta_0 \) är väntevärdet som gäller för nollhypotesen och \( D \) är standardavvikelsen för skattningen.</p>
<p><strong>signifikanstest</strong> 1. Om \(t_{obs} \in \text{jätteosannolikt område} \) förkasta \( H_0 \) 2. Om \( t_{obs} \) å andra sidan är ett sannolikt utfall, även i vanliga fall så bör \( H_0 \) inte förkastas.</p>
<p><strong>direkt- och P-värdesmetoden</strong> – anta \( H_0 \) och beräkna sannolikheten att man får det utfall man fått eller något värre. “Värre” betyder alla utfall som är ännu mindre sannolika om \( H_0 \) stämmer.</p>
<p><strong>konfidensmetoden</strong> – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför.</p>
<p><strong>Testkvantitet</strong> – använd testvariabel och tabell och se om testvariabeln är större eller mindre än en viss α-kvantil.</p>
<p><strong>styrkefunktionen</strong> \( h(\theta) = P(H_0 \text{ förkastas}) \) om θ är det rätta värdet.</p>
<ul>
<li>bör vara stort för alla θ som tillhör mothypotesen</li>
<li>bör vara litet för alla θ som tillhör nollhypotesen</li>
<li>h(θ) kallas testets styrka för θ</li>
</ul>
<h4 id="how-to-styrkefunktion">How to styrkefunktion</h4>
<ol type="1">
<li>\( X \in N( \theta, 0.04) \) Antag \( H_0: \theta = 2.0 \) och sätt upp villkoret för att förkasta \( H_0 \). e.g. \( | X - 2.0 | \geq 0.04 \lambda_{0.025} \)</li>
<li>Skapa nu styrkefunktionen \(h(\theta) = P(| X - 2.0 | \geq 0.04 \lambda_{0.025}) = 1 - P(-0.04 \lambda_{0.025} \leq X - 2.0 \leq 0.04 \lambda_{0.025}) \). Denna duger inte än eftersom den inte beror av θ.</li>
<li>Då styrkefunktionen alltså går ut på att få se sannolikheten att \( H_0 \) förkastas när \( E(X) = \theta \) antar vi att \( E(X) = \theta \) och bildar variabelbytet \( u = \frac{X - \theta}{0.04} \) för att kunna använda den standardiserade normalfördelningen.</li>
<li>Via enklare räkning byter vi ut \( (X - 2.0)/D \) mot (\( u \) och kan då använda den standardiserade nomralfördelningens fördefördelningsfunktion \[ h( \theta ) = 1 - \Phi ( \lambda_{0.025} + \frac{2.0 - \theta}{0.04}) + \Phi (- \lambda_{0.025} + \frac{2.0 - \theta}{0.04}) \]</li>
</ol>
<h3 id="regressionsanalys">10. Regressionsanalys</h3>
<p>När man vill se samband mellan två eller flera storheter.</p>
<h4 id="observera">observera!</h4>
<ul>
<li>Använd alltid t-fördelningen när standardavvikelsen blivit skattad.</li>
</ul>
<p><strong>Icke linjära samband</strong> – Logaritmera \[ y_i = c e^{\beta_{t_i}} \epsilon_i \] \[ ln y_i = ln c + \beta t_i + ln \epsilon_i \] Glöm nu inte att använda \( ln y_i \), alltså inte \( y_i \). Samma gäller alla sådana termer.</p>
<h4 id="terminologi-2">Terminologi</h4>
<p><strong>teoretiska regressionslinjen</strong> \[ y = \alpha + \beta x \]</p>
<p><strong>parameterskattningar</strong></p>
<p>\[ \sum x_i , \quad \sum x_i^2 , \quad S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - \frac{1}{n} (\sum_{i=1}^n x_i)^2 \]</p>
<p>\[ \sum y_i , \quad \sum y_i^2 , \quad S_{yy} = \sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n y_i^2 - \frac{1}{n} (\sum_{i=1}^n y_i)^2 \]</p>
<p>\[ \sum x_i y_i , \quad S_{xy} = \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}) = \sum_{i=1}^n x_i y_i - \frac{1}{n} (\sum_{i=1}^n x_i) (\sum_{i=1}^n y_i) \]</p>
<h4 id="punktskattningar">Punktskattningar</h4>
<p>Remember MK-metoden? Bestäm minimum för</p>
<p>\[ Q( \alpha , \beta ) = \sum_{i}^n (y_i - \mu_i)^2 \] \[ \mu_i = \alpha + \beta x_i \]</p>
<p>Genom att sätta partialderivatorna till noll fås</p>
<p>\[ \beta^* = \frac{S_{xy}}{S_{xx}} \quad \alpha^* = \bar{y} - \beta^* \bar{x} \]</p>
<p>\[ ( \sigma^2 )^* = s^2 = \frac{Q_0}{n-2}, \quad Q_0 = S_{yy} - S_{xy}^2 / S_{xx} \]</p>
<p>Observera att</p>
<p>\[ \mu_0^* = \alpha^* + \beta^* x_0 \in N( \alpha + \beta x_0 , \sigma \sqrt{ \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}) \]</p>
<h4 id="intervallskattningar">Intervallskattningar</h4>
<p><strong>Prediktionsintervall för observationer</strong> – När man vill göra ett konfidensintervall för framtida observationer av \( Y \) för \( x = x_0 \)</p>
<p><strong>Kalibreringsintervall</strong> – Då man erhållit värde \( y_0 \text{ på } y \), vad blir då \( x_0 \)</p>
<h4 id="stokastiska-vektorer">Stokastiska vektorer</h4>
<p><strong>stokastisk vektor</strong> – Kolonnvektor vars element är stokastiska variabler. Den kan alltså representera en n-dimensionell stokastisk variabel. \[ X = [X_1,…,X_n]^T \]</p>
<p><strong>väntevärdesvektorn</strong> – Elementvis beräknade väntevärden för \( X \)</p>
<p><strong>kovariansmatrisen</strong> – Också en funktion på \( X \). Denna genererar \( C(X_i,X_j) \) på \( X_{ij} \). Då \( C(X,X) = V(X) \) hittar man varianser för de olika s.v. på diagonalelementen.</p>
<h4 id="multipel-regression">Multipel regression</h4>
<p>För stilrenhet döps \( \alpha \) om till \( \beta_0 \).</p>
<p>\[ y_i = \beta_0 + \beta_1 x_{i1} + … + \beta_k x_{ik} + \epsilon_i, \quad i=1,…,n \]</p>
<p>Skriv modellen på matrisform för att generalisera ytterligare. \[ y = X \beta + \epsilon \]</p>
<p><strong>QUICKGUIDE:</strong> – en guide för formelsamlingsägare: 1. Beräkna \( \beta^* \). 2. Beräkna \( s^2 = \frac{Q_0}{n-(p+1)} \)</p>
<p><strong>TIPS:</strong> – för dig som vill bli regressiv king</p>
<ul>
<li><p><strong>Välj många olika kombinationer</strong> av \( x_{ij} \). – Om alla \( x_{ik} \) är samma kommer normalekvationen sakna entydlig lösning eftersom normalen då balanserar på en linje av mätdata och och inte är utspridd över axeln tillhörande \( x_{ik} \).</p></li>
<li><p><strong>Håll koll på (n - (p + 1))</strong> – då summan återkommer i variansskattning och som antal frihetsgrader till t-fördelningen. <strong>p</strong> är antalet olika x-variabler och därav också antalet x-dataserier med <strong>n</strong> värden i varje serie.</p></li>
</ul>
</body>
</html>
