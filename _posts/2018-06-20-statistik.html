<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Sannolikhetsteori och statistikteori</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Sannolikhetsteori och statistikteori</h1>
</header>
<h3 id="del-1-sannolikhet-eller-hur-man-beskriver-slumpen">DEL 1: Sannolikhet eller hur man beskriver slumpen</h3>
<p><em>utfall</em> – resultatet av ett slumpmässigt försök</p>
<p><em>utfallsrummet</em> – mängden möjliga utfall</p>
<p><em>händelse</em> – samling utfall</p>
<p><em>relativ frekvens</em> – kvoten mellan antalet erhållet utfall och hela antalet utförda kast</p>
<p><em>disjunkta händelser</em> – kan inte inträffa samtidigt</p>
<p><em>Kolmogorovs axiomsystem</em>:</p>
<ol type="1">
<li>Händelsen P(A) måste ligga mellan 0 &amp; 1</li>
<li>P(utfallsrummet) = 1</li>
<li>om A &amp; B är parvis oförenliga gäller \( P(A) + P(B) = P(A \cup B) \)</li>
</ol>
<p><em>komplementsatsen</em> P(A*) = 1 - P(A)</p>
<p><em>Additionssatsen</em> P(A or B) = P(A) + P(B) - P(A and B)</p>
<p><em>Booles olikhet</em> – <span class="math display">\[ P(A \cup B) \leq P(A) + P(B) \]</span></p>
<h5 id="kombinatorik">Kombinatorik</h5>
<p>Förutsättningar:</p>
<ul>
<li>n element</li>
<li>k av dessa plockas</li>
</ul>
<p><em>Klassiska sannolikhetsdefinitionen</em> Vid likformigt sannolikhetmått är sannolikheten för en händelse lika med kvoten mellan antalet för händelsen gynsamma fall och antalet möjliga fall.</p>
<p><em>Dragning med återläggning och hänsyn till ordning</em> \[n^k\]</p>
<p><em>Dragning utan återläggning med hänsyn till ordning</em> \[n*(n-1)(n-2) \cdots (n-k)\]</p>
<p><em>Dragning utan återläggning utan hänsyn till ordning</em> \[\binom{n}{k}\]</p>
<p><em>Dragning utan återläggning</em> Urna med kulor av två olika färger. Hur stor är chansen att erhålla k vita? Enl. <em>Klas. sann.</em> ges svaret av \[ g/m \] \[ m = \binom{v+s}{n} \] \[ g = \binom{v}{k} \binom{s}{n-k} \] Alltså produkten av sätten att få k stycken vita och alla möjligheter att få resterande svarta.</p>
<p><em>Dragning med återläggning</em> Samma som ovan men med återläggning. \[ m = (v+s)^n \] \[ g = \binom{n}{k} v^k s^{n-k}\]</p>
<p>Alltså antalet olika kombinationer det finns av k stora samlingar bland n multiplicerat med sannolikheten för k vita multiplicerat med n-k svarta. Allt detta dividerat med m.</p>
<p><em>Betingade sannolikheten</em> – sannolikheten att något inträffar givet en annan händelse.</p>
<p>\[P(B|A) = \frac{P(A \cap B)}{P(A)}\]</p>
<p>Ger alltså ett samband mellan betingning och snitt.</p>
<p><em>Lagen om total sannolikhet</em> \[P(A) = \sum_{i=1}^{n} P(H_i)P(A|H_i)\]</p>
<p><em>Bayes sats</em> \[P(H_i|A) = \frac{P(H_i)P(A|H_i)}{\sum_{j=1}^{n} P(H_j)P(A|H_j)}\]</p>
<p><em>Oberoende händelser</em> – P(B|A) = P(B)</p>
<p><em>sannolikheten att minst en inträffar</em> \[A_1 , A_2 , … , A_n är oberoende och P(A_i)=p_i\] \[1-(1-p_1)(1-p_2)…(1-p_n) = 1-(1-p)^n\]</p>
<h3 id="endimensionella-stokastiska-variabler">1. Endimensionella stokastiska variabler</h3>
<p>Den stokastiska variabeln är bron mellan matematiken och slumpen men är inget mer än en reellvärd funktion definierad på ett utfallsrum. Betecknas i texten som versaler från slutet av alfabetet som X, Y, eller Z.</p>
<h4 id="diskret-stokastisk-variabel">diskret stokastisk variabel</h4>
<p>En s.v. är <em>diskret</em> om den kan anta ett ändligt eller uppräkneligt oändligt antal olika värden. funktionen över värdemängden kallas sannolikhetsfunktionen.</p>
<ol type="1">
<li><p><em>Enpunktsfördelning</em> – all massa i ett värde \[p_X(a) = 1\]</p></li>
<li><em>Tvåpunktsfördelning</em> – om X endast antar två värden a &amp; b med sannolikheterna p respektive 1-p.</li>
</ol>
<ul>
<li>ex: krona/klave</li>
<li>då X tar värdena a = 1 och b = 0 sägs X vara Bernoulli-fördelad.</li>
</ul>
<ol start="3" type="1">
<li><em>Likformig fördelning</em> – X antar värden 1,2,..,m och alla dessa med samma sannolikhet.</li>
</ol>
<ul>
<li>\[p_X(k)=1/m, k = 1,2,…,m.\]</li>
</ul>
<ol start="4" type="1">
<li><em>För-första-gången-fördelning</em></li>
</ol>
<ul>
<li>\[ p_X(k)=(1-p)^{k-1}p, k=1,2,…,\]</li>
<li>När samma oberoende försök görs om och om tills ett visst resultat erhålls. Antalet försök t.o.m. resultatet är då en s.v. med ffg-fördelning.</li>
<li>\[ X \in ffg(p) \]</li>
</ul>
<ol start="5" type="1">
<li><em>Geometrisk fördelning</em> – genom att skippa resultatrundan som räknas in i ffg-fördelningen tillhör X Ge(p)</li>
</ol>
<ul>
<li>\[p_X(k) = (1 - p)^kp, k = 0,1,2,…,\]</li>
</ul>
<ol start="6" type="1">
<li><strong>Binomialfördelning</strong> – slumpmässigt försök med en händelse A där P(A) = p upprepas n oberoende ggr.</li>
</ol>
<ul>
<li>\[ p_X(k) = \binom{n}{k}p<sup>k(1-p)</sup>{n-k} \]</li>
<li>\[ X \in Bin(n,p) \]</li>
</ul>
<ol start="7" type="1">
<li><strong>Hypergeometrisk fördelning</strong> – uppträdde vid dragning utan återläggning ur urna med vita och svarta kulor.</li>
</ol>
<ul>
<li>\[ p_X(k) = \frac{\binom{v}{k} \binom{s}{n-k}}{\binom{v+s}{n}}\]</li>
<li>\[ X \in Hyp(N,n,p)\]</li>
</ul>
<ol start="8" type="1">
<li><strong>Poisson-fördelning</strong></li>
</ol>
<ul>
<li>\[ p_X(k) = \frac{\mu<sup>k}{k!}e</sup>{-e}\]</li>
<li>\[ X \in Po(\mu)\]</li>
</ul>
<h4 id="kontinuerlig-stokastisk-variabel">kontinuerlig stokastisk variabel</h4>
<p>Sannolikhetsfunktionen kallas nu täthetsfunktion och betecknas med f.</p>
<ol type="1">
<li><strong>Likformig fördelning</strong> – \[ f_X(x) = 1/(b-a) om a &lt; x &lt; b &gt;\]</li>
<li><strong>Exponentialfördelning</strong> – beskriver tiderna mellan händelserna i en poissonprocess. \[ f_X(x) = \lambda e^{-\lambda x} \quad E(X) = 1 / \lambda, \quad D(X) = 1 / \lambda \]</li>
<li><strong>Normalfördelningen</strong></li>
<li><strong>Weibull-fördelning</strong></li>
<li><strong>Gammafördelning</strong></li>
</ol>
<p><strong>fördelningsfunktion</strong></p>
<p><strong>intensitet</strong></p>
<h3 id="flerdimensionella-stokastiska-variabler">2. Flerdimensionella stokastiska variabler</h3>
<h4 id="note-to-self">Note to self:</h4>
<ul>
<li>\[ p_{X,Y}(i,j) = p_X(i)p_Y(j) = P(j|i)P(i) \] OBS! Dont forget the last P(i)</li>
</ul>
<h4 id="största-och-minsta-värdet">Största och minsta värdet</h4>
<ul>
<li>Z = max(X,Y)
<ul>
<li>\[F_Z(z) = F_X(z)F_Y(z)\]</li>
</ul></li>
<li>Z = min(X,Y)
<ul>
<li>\[F_Z(z) = 1-[1-F_X(z)][1-F_Y(z)]\]</li>
</ul></li>
</ul>
<p>Gör först om till fördelningsfunktion om täthetsfunktion</p>
<h4 id="summan-av-s.v.">Summan av s.v.</h4>
<ul>
<li>\[f_Z(z) = \int_{\infty}^{\infty} f_X(x)f_Y(z-x)dx\]</li>
</ul>
<h3 id="väntevärden">3. Väntevärden</h3>
<h4 id="note-to-self-1">Note to self:</h4>
<p>konstanter inuti väntevärdesfunktioner är korkat.</p>
<h4 id="väntevärdet-ex-eller-μ">Väntevärdet E(X) eller μ</h4>
<p>Är ett typ av lägesmått, precis som medianen. E(X) är väntevärdet för X. E(X) berättar om vad det väntade resultatet blir.</p>
<p><strong>DEF:</strong> \[ E(X) = \sum_kkp_X(k) \] \[ E(X) = \int\limits_{-\infty}^{\infty} x f_X(x)dx \]</p>
<p><strong>Y = g(X)</strong> – Väljer du att interfacea funktionen med en ny variabel som beror av X med samma gamla fördelning så kan man trixa enl. följande \[ E(Y) = \sum_kg(k)p_X(k)\] \[ E(X) = \int\limits_{-\infty}^{\infty} g(x) f_X(x)dx \]</p>
<p>\[ E(X+Y) = E(X)+E(Y) \]</p>
<p>X &amp; Y oberoende \[ E(XY) = E(X)E(Y)\]</p>
<p>Samling X med samma väntevärde µ \[ E(\sum_{i=1}^nX_i)=n\mu \]</p>
<h5 id="betingade-väntevärden">Betingade väntevärden</h5>
<p>\[ E(X|Y=k) = \sum_{j=0}^\infty jp_{X|Y=k}(j)\]</p>
<p>\[ E(X|Y=y) = \int_{-\infty}^\infty xf_{X|Y=y}(x)dx\]</p>
<h4 id="variansen-vx-eller-σ">Variansen V(X) eller σ²</h4>
<p>Variansen är en typ av spridningsmått.</p>
<p><strong>DEF:</strong> \[ V(X) = E[(X-\mu)^2] \]</p>
<p><strong>Schysta satser:</strong> <span class="math display">\[ V(X) = E(X^2)-[E(X)]^2 \]</span> \[ V(aX+b) = a^2V(X) \] \[ V(X + Y) = V(X) + V(Y) + 2C(X,Y) \]</p>
<ul>
<li>Om oberoende: \( V(X + Y) = V(X) + V(Y) \)</li>
<li>Om oberoende och med samma σ: \( V(\sum_{i=1}^nX_i) = n\sigma^2 \)</li>
<li>Om oberoende och med samma σ samt µ: \( V(\bar{X})=\sigma^2/n \)</li>
</ul>
<p>Tänk på att använda formelsamlingen för att snabbt fastställa varians</p>
<h4 id="standardavvikelse-dx-eller-σ">Standardavvikelse D(X) eller σ</h4>
<p>Schyst mått då man får samma dimension som väntevärdet \[ D(X) = \sqrt{V(X)} \] \[ D(aX + b) = |a|D(X) \] Om oberoende: \[ D(X + Y) = \sqrt{D^2(X) + D^2(Y)} \]</p>
<h4 id="variationskoefficienten">Variationskoefficienten</h4>
<p>uttrycks i procent \[ R(X) = D(X)/E(X) \]</p>
<h4 id="fel">fel</h4>
<ul>
<li><em>systematiskt fel/bias</em> är differansen mellan mätvärdets väntevärde och det korrekta värdet. (ett tal)</li>
<li><em>slumpmässigt fel</em> menas differensen mellan mätvärdet och dess väntevärde. (s.v. med E(X) = 0)</li>
</ul>
<h4 id="beroendemått">Beroendemått</h4>
<h5 id="kovarians">Kovarians</h5>
<ul>
<li>Kovariansen C(X,Y) mellan X &amp; Y bör bli positiv om det finns ett beroende sådant att det finns en tendens hos variablerna att samtidigt avvika åt samma håll från sina väntevärden.</li>
<li>\( C(X,Y) = E[(X-\mu_X)(Y-\mu_Y)] \)</li>
<li>\( C(X,Y) = E(XY)-E(X)E(Y) \)</li>
<li>Om C(X,Y) = 0 är X och Y okorrelerade.</li>
<li>\( X \text{ &amp; } Y \text{ oberoende} \to \text{okorrelerade} \)</li>
</ul>
<h5 id="korrelationskoefficienten">Korrelationskoefficienten</h5>
<ul>
<li>DEF: \[ \rho(X,Y) = \frac{C(X,Y)}{D(X)D(Y)} \]</li>
<li>Kovarians fast dimensionslös</li>
</ul>
<h4 id="stora-talens-lag">Stora talens lag</h4>
<ul>
<li>Ju fler oberoende s.v. med samma µ desto närmre kommer medelvärdet att gå mot µ.</li>
</ul>
<h4 id="betingade-väntevärden-och-varianser">Betingade väntevärden och varianser</h4>
<h4 id="gauss-approximationsformler">Gauss approximationsformler</h4>
<p>Har du någonsin känt dig inkapabel? Då är taylorutveckling något för dig! Allt för ofta vill man ha en schyst funktion mitt i väntevärdet men hur räknar man ut E(Y) då!? Du behöver inte vara helt körd i skallen, det kan vara så att du råkat ut för någon av de många fallgropar som kantar väntevägen!</p>
<h5 id="en-variabel">En variabel</h5>
<ol type="1">
<li>taylorutveckla: \[ g(X) \approx g(\mu) + (X - \mu)g’(\mu) \]</li>
<li>g(X) har nu approximativa väntevärdet g(µ) samt [g’(E(X))]²V(X) som varians. Med en rak linje blir det enkelt att räkna med µ och σ².</li>
</ol>
<h5 id="flera-variabler">Flera variabler</h5>
<h6 id="abandon-all-hope-ye-who-enter-here">Abandon all hope, ye who enter here</h6>
<ol type="1">
<li>taylorutveckla: \[ g(X,Y) \approx g(\mu_X,\mu_Y)+(X-\mu_X)g’_X(\mu_X,\mu_Y)+(Y-\mu_Y)g’_Y(\mu_X,\mu_Y) \]</li>
</ol>
<h3 id="normalfördelningen">4. Normalfördelningen</h3>
<h5 id="notes-to-self">Notes to self:</h5>
<ul>
<li>ca en tredjedel av massan hamnar utanför en standardavvikelse.</li>
<li>normalfördelningar bevaras alltid under linjära transformationer</li>
</ul>
<p><span class="math display">\[ f_X(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]</span></p>
<h4 id="standardiserad-fördelning">Standardiserad fördelning</h4>
<p>Täthetsfunktion: φ Fördelningsfunktion: Φ</p>
<p>\[ \Phi(-x) = 1 - \Phi(x) \]</p>
<h4 id="allmän-fördelning">Allmän fördelning</h4>
<p>\[ X \in N(\mu,\sigma) \quad iff \quad Y = (X-\mu)/\sigma \in N(0,1) \] \[ f_X(x) = \frac{1}{\sigma}\varphi(\frac{x-\mu}{\sigma}) \] \[ F_X(x) = \Phi(\frac{x-\mu}{\sigma}) \]</p>
<h4 id="linjärkombinationer">Linjärkombinationer</h4>
<p>Om \[ X \in N(\mu,\sigma) \] så gäller att \[ Y=aX+b \in N(a\mu + b, |a|\sigma) \]</p>
<p>Om \[ X_1,X_2,..,X_n \] är oberoende N(µ,σ) och \[ \bar{X} \] är medelvärdet så gäller att \[ \bar{X} \in N(\mu,\sigma/\sqrt{n}) \]</p>
<h3 id="de-tre-vännerna-och-binomialfördelning">5. De tre vännerna och Binomialfördelning</h3>
<h4 id="binomialaren-med-återläggning">binomialaren (med återläggning)</h4>
<p>E(X) = np V(X) = npq</p>
<p>Om oberoende \[ X \in Bin(n_1,p) \quad \&amp; Y \in Bin(n_2,p) \] \[ X + Y \in Bin(n_1+n_2,p) \]</p>
<p><strong>Obs! Glöm inte att bin är diskret, håll därför koll på gränserna (&gt; != &gt;=)</strong></p>
<p>Kan approximeras som</p>
<ol type="1">
<li><em>poissonfördelning</em> om p är litet</li>
<li><em>normalfördelning</em> om n är stort N(np,sqrt(npq))</li>
</ol>
<h4 id="hypergeometriske-utan-återläggning">Hypergeometriske (utan återläggning)</h4>
<p>E(X) = np V(X) = ((N-n)/(N-1))np(1-p)</p>
<p>Kan aproximeras som</p>
<ol type="1">
<li><em>binomialapproximation</em> om n/N är liten</li>
<li><em>normalapproximation</em> om n är stort</li>
</ol>
<h4 id="poisson-fördelningen">Poisson-fördelningen</h4>
<p>E(X) = µ V(X) = µ</p>
<p>\[ X_1 \in Po(\theta_1) \quad and \quad X_2 \in Po(\theta_2) \quad then \quad X_1+X_2 \in Po(\theta_1+\theta_2) \]</p>
<p>Kan approximeras som</p>
<ol type="1">
<li><em>normalfördelning</em> om µ är stort</li>
</ol>
<h4 id="multinomial">Multinomial</h4>
<h3 id="slumptal">6. Slumptal</h3>
<h3 id="markovkedjor">Markovkedjor</h3>
<p>Stokastiska processer vars nästa värde endast beror på nuvarande värde.</p>
<p>övergångsmatris används för att skriva upp “hoppsannolikheterna”.</p>
<p>övergångssannolikheter av 2a ordningen härleds genom att matrismultiplicera övergångsmatrisen med sig själv. alltså sannolikheten att mellanlanda i ett tillstånd.</p>
<p>För att simulera sannolikheterna att systemet börjar i de olika tillstånden används matrismultiplikation med en radvektor <span class="math display">\[ p^{(0)}=(p_1^{(0)},p_2^{(0)},...) \]</span></p>
<p>\[ \begin{pmatrix} 1 &amp; 2 \end{pmatrix} * \begin{pmatrix} 1to1 &amp; 1to2 \ 2to1 &amp; 2to2 \end{pmatrix} \]</p>
<h5 id="terminologi">terminologi</h5>
<p><strong>beständigt</strong> tillstånd om P(i-&gt;i)=1 <strong>obeständigt</strong> tillstånd om P(i-&gt;i) less than 1</p>
<p><strong>Om två tillstånd kommunicerar tvåsidigt är de båda antingen beständinga eller inte.</strong></p>
<p><strong>irreducibel</strong> om alla tillstånd kommunicerar tvåsidigt med varandra, indirekta anslutningar räknas också.</p>
<p><strong>stationär fördelning</strong> sannolikheterna att systemet befinner sig i de olika tillstånden.</p>
<ol type="1">
<li>skapa sannolikhetsvektorn π = (π1,π2,..)</li>
<li>lös ekv. π = πP (P är övergångsmatrisen)</li>
</ol>
<p>när <span class="math display">\[ p^{(n)}=(p_1^{(n)},p_2^{(n)},...) \to \pi \]</span> när \[ n \to \infty \] 1. om man i en ändlig kedja kan finna ett r&gt;0 så beskaffat att alla element i någon kolonn i matrisen P^r är positiva, existerar det en asymptotisk fördelning. 2. se stationär fördelning</p>
<p><em>periodiska tillstånd</em> om det alltid krävs ett visst antal hopp för att komma tillbaka till ett tillstånd är tillståndet periodiskt. t.ex. om processen bara kan nå tillbaka till Ei efter 3,6,9,… steg har Ei perioden 3.</p>
<p><em>aperiodiska tillstånd</em> om det alltid går att komma tillbaka till ett tillstånd direkt</p>
<hr />
<h3 id="del-2-statistik-eller-vilka-slutsatser-man-kan-dra-av-ett-datamaterial">DEL 2: Statistik eller vilka slutsatser man kan dra av ett datamaterial</h3>
<h4 id="terminologi-1">terminologi</h4>
<p><em>parameterrummet</em> - de värden den sökta parametern kan tänkas anta.</p>
<p><em>stickprov</em> – betecknas med lilla x = (x1,x2,…,xn) för n dimensionella s.v.</p>
<p><em>stickprovsvariansen</em> \[ s^2 = \frac{1}{n-1} \sum_{j=1}^n (x_j - \bar{x})^2 \]</p>
<p><em>kovariansen mellan x- och y-värdena i en datamängd (x1,y1),(x2,y2),…,(xn,yn)</em> \[ c_{xy} = \frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) \] <em>korrelationskoefficienten</em> \[ r = \frac{c_{xy}}{s_xs_y} \]</p>
<h3 id="punktskattning">7. Punktskattning</h3>
<p><em>punktskattning</em> – den observerade sannolikheten – ett utfall av stickprovsvariabeln \[ \theta_{obs}^*(x_1,x_2,…,x_n) \]</p>
<p><em>stickprovsvariabeln</em> – en s.v. som punktskattningen är ett utfall av \[ \theta^*(X_1,X_2,…,X_n) \]</p>
<p><em>väntevärdesriktig</em> – punktskattning vars tillhörande stickprovsvariabel har väntevärdet θ. dvs om \[ E(\theta^*) = \theta \]</p>
<p><em>MSE</em> – mean square error – medelkvadratfelet för en punktskattning – mått på slumpmässigt fel \[ MSE = E(( \theta^* - \theta)^2) \]</p>
<h4 id="skattning-av-μ-σ">skattning av μ &amp; σ</h4>
<h5 id="µ">µ</h5>
<p>stickprovsmedelvärdet \[ \bar{x} \] är en väntevärdesriktig och konsistent skattning av µ</p>
<h5 id="σ2">σ^2</h5>
<p>stickprovsvariansen s^2 är en väntevärdesriktig skattning av σ^2</p>
<h4 id="maximum-likelihood-metoden-ml-metoden">Maximum-likelihood-metoden – ML-metoden</h4>
<ol type="1">
<li>Skapa \[ L(\theta) = P(X_1 = x_1, X_2 = x_2,…,X_n = x_n;\theta) \] alt. \[ L(\theta) = f_{X_1,X_2,…,X_n}(x_1,x_2,…,x_n;\theta) \] (A.k.a. likelihood-funktionen)</li>
<li>Finn funktionens maxpunkt genom ex. derivering över theta.</li>
<li>Funktionens största värde är det mest sannolika scenariot.</li>
</ol>
<h4 id="minsta-kvadrat-metoden-mk-metoden">Minsta-kvadrat-metoden – MK-metoden</h4>
<p>\[ Q(\theta) = \sum_{i=1}^n [x_i - \mu_i (\theta)]^2 \] Går ut på att anta att det finns små försöksfel vid varje mätdatum och bara genom att minimera dessa finner man bästa skattning av theta. Så hur går man då tillväga? Jo genom att skapa funktionen \( Q( \theta )) \) och sedan derivera denna och finna minimum för \[ \frac{dQ}{d \theta} Q( \theta ) \]</p>
<p>Flera parametrar? Lös partialderivatorna och sedan ekvationssystemet.</p>
<h3 id="intevallskattning">8. Intevallskattning</h3>
<p>När man vill veta hur stor sannolikhet det är att en okänd parameter ligger inom ett visst interval.</p>
<h4 id="tillämpning-på-normalfördelningen">Tillämpning på normalfördelningen</h4>
<h4 id="ett-stickprov">Ett stickprov</h4>
<h5 id="µ-okänt-σ-känt">µ okänt σ känt</h5>
<p>\[\mu* = \bar{x} \]</p>
<h5 id="μ-känt-σ-okänt">μ känt σ okänt</h5>
<p><span class="math display">\[ (\sigma^2)_{obs}^* = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2 \]</span></p>
<h5 id="konfidensintervall-för-väntevärdet">Konfidensintervall för väntevärdet</h5>
<h5 id="känd-standardavvikelse">Känd standardavvikelse</h5>
<p>En lämplig skattning av µ är aritmetiska medelvärdet av X. \[ \bar{X} \in N(\mu,D) \] \[ D = \sigma/\sqrt{n} \] \[ I_\mu = (\bar{x}-\lambda_{\alpha/2}D,\bar{x}+\lambda_{\alpha/2}D) \]</p>
<p>Allt detta följer av att:</p>
<p>\[ \frac{\bar{X}-\mu}{D} \in N(0,1) \]</p>
<p>Följaktligen gäller med sannolikheten 1-alfa att:</p>
<p>\[ -\lambda_{\alpha/2} &lt; \frac{\bar{X}-\mu}{D} &lt; \lambda_{\alpha/2} &gt;\]</p>
<p>Om vi har ett intervall:</p>
<p>\[ I_\mu = (16 \pm 2.58 * 0.155) \]</p>
<p>där</p>
<p>\[ D = 1.2/\sqrt{60} = 0.155 \]</p>
<p>och man istället vill ha en mindre standardavvikelse, säg 0.5, så kan man sätta upp följande ekvation:</p>
<p>\[ 2 * 2.58 * 1.2/\sqrt{n} = 0.5 \]</p>
<h5 id="okänd-standardavvikelse">Okänd standardavvikelse</h5>
<p>I detta fallet gäller en helt galen lösning eftersom man behöver skatta σ</p>
<p>\[ I_\mu = (\bar{x}-t_{\alpha/2}(f)d,\bar{x}+t_{\alpha/2}(f)d) \] \[ d = s/\sqrt{n}, \quad f = n-1 \]</p>
<h5 id="konfidensintervall-för-standardavvikelsen">Konfidensintervall för standardavvikelsen</h5>
<h5 id="känt-väntevärde">Känt väntevärde</h5>
<p>Aint gonna happen gurl</p>
<h6 id="okänt-väntevärde">Okänt väntevärde</h6>
<p>\[ I_\sigma = (k_1s,k_2s) \] \[ k_1 = \sqrt{(f/\chi_{\alpha/2}^2(f)} \] \[ k_2 = \sqrt{(f/\chi_{1-\alpha/2}^2(f)} \] \[ f = n-1 \]</p>
<h4 id="två-stickprov">Två stickprov</h4>
<p>Om σ1 och σ2 är kända:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-\lambda_{\alpha/2}D,\bar{x}-\bar{y}+\lambda_{\alpha/2}D) \] \[ D = \sqrt{ \sigma_{1}^{2} / n_1 + \sigma_{2}^{2} / n_2} \]</p>
<p>Om σ1 = σ2 = σ:</p>
<p>\[ I_{\mu_1-\mu_2} = (\bar{x}-\bar{y}-t_{\alpha/2}(f)d,\bar{x}-\bar{y}+t_{\alpha/2}(f)d) \] \[ d = \sigma \sqrt{ \frac{1}{n_1} + \frac{1}{n_2}} \]</p>
<h4 id="stickprov-i-par">Stickprov i par</h4>
<p>Skapa \[ z = y - x \]</p>
<h3 id="hypotesprövning">9. Hypotesprövning</h3>
<p><strong>nollhypotes</strong> – hypotesen att det inte föreligger något fenomen som kräver en förklaring. Betecknas: \( H_0 \)</p>
<p><strong>mothypotes</strong> – hypotes som kan vara sann om inte nollhypotesen är det. Betecknas: \( H_i \)</p>
<p><strong>signifikansnivå/felrisk</strong> – sannolikheten att nollhypotesen förkastas trots att den är sann. (Ju lägre desto bättre).</p>
<ul>
<li>signifikant* – 0.05</li>
<li>signifikant** – 0.01</li>
<li>signifikant*** – 0.001</li>
</ul>
<p><strong>testvariabel/teststorhet</strong> – observation av stickprovsvariabel</p>
<p><strong>signifikanstest</strong> 1. Om \(t_{obs} \in \text{jätteosannolikt område} \) förkasta \( H_0 \) 2. Om \( t_{obs} \) å andra sidan är ett sannolikt utfall, även i vanliga fall så bör \( H_0 \) inte förkastas.</p>
<p><strong>konfidensmetoden</strong> – genom att beräkna konfidensintervall för variabel och sedan förkasta nollhypotesen om värdet hamnar utanför</p>
<p><strong>styrkefunktionen</strong> \[ h(\theta) = P(H_0 \text{ förkastas}) \] om θ är det rätta värdet</p>
<ul>
<li>bör vara stort för alla θ som tillhör mothypotesen</li>
<li>bör vara litet för alla θ som tillhör nollhypotesen</li>
<li>h(θ) kallas testets styrka för θ</li>
</ul>
<h4 id="how-to-styrkefunktion">How to styrkefunktion</h4>
<ol type="1">
<li>create \( u = \frac{x - \mu}{\sigma} \)</li>
<li>make sure \( u &gt; \lambda_\alpha \) as this means that it is less likely it happens than α</li>
<li>solve for x</li>
<li>create \( h(\theta) = P(x &gt; \lambda_\alpha \sigma + \mu) \)</li>
<li>normalize h(θ) by adding -μ and dividing it all by σ</li>
</ol>
<h3 id="regressionsanalys">10. Regressionsanalys</h3>
<p>När man vill se samband mellan två eller flera storheter.</p>
<h4 id="terminologi-2">Terminologi</h4>
<p><strong>teoretiska regressionslinjen</strong> \[ y = \alpha + \beta x \]</p>
<p><strong>parameterskattningar</strong></p>
<p>\[ \sum x_i , \quad \sum x_i^2 , \quad S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2 = \sum_{i=1}^n x_i^2 - \frac{1}{n} (\sum_{i=1}^n x_i)^2 \]</p>
<p>\[ \sum y_i , \quad \sum y_i^2 , \quad S_{yy} = \sum_{i=1}^n (y_i - \bar{y})^2 = \sum_{i=1}^n y_i^2 - \frac{1}{n} (\sum_{i=1}^n y_i)^2 \]</p>
<p>\[ \sum x_i y_i , \quad S_{xy} = \sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}) = \sum_{i=1}^n x_i y_i - \bar{x} \bar{y} \]</p>
<h4 id="punktskattningar">Punktskattningar</h4>
<p>Remember MK-metoden? Bestäm minimum för</p>
<p>\[ Q( \alpha , \beta ) = \sum_{i}^n (y_i - \mu_i)^2 \] \[ \mu_i = \alpha + \beta x_i \]</p>
<p>Genom att sätta partialderivatorna till noll fås</p>
<p>\[ \beta^* = \frac{S_{xy}}{S_{xx}} \quad \alpha^* = \bar{y} - \beta^* \bar{x} \]</p>
<h5 id="skattning-av-σ">Skattning av σ</h5>
<p>\[ ( \sigma^2 )^* = s^2 = \frac{Q_0}{n-2}, \quad Q_0 = S_{yy} - S_{xy}^2 / S_{xx} \]</p>
<p>Observera att</p>
<p>\[ \mu_0^* = \alpha^* + \beta^* x_0 \in N( \alpha + \beta x_0 , \sigma \sqrt{ \frac{1}{n} + \frac{(x_0 - \bar{x})^2}{S_{xx}}}) \]</p>
<h4 id="intervallskattningar">Intervallskattningar</h4>
<h3 id="fallgropar">11. Fallgropar</h3>
</body>
</html>
